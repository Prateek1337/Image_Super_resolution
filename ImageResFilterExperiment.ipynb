{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"ImageResFilterExperiment.ipynb","provenance":[{"file_id":"https://github.com/Prateek1337/Image_Super_resolution/blob/master/basic.ipynb","timestamp":1580919513704}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"qTLHSmW23asG","colab_type":"code","outputId":"baa3132b-0791-45dc-8552-008ecc4f4d74","executionInfo":{"status":"ok","timestamp":1580906910477,"user_tz":-330,"elapsed":23179,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":148}},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","drive  sample_data\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zV7UDX6B3q7P","colab_type":"code","outputId":"e0912f61-32e8-4d89-f5d4-a715d7755f4a","executionInfo":{"status":"ok","timestamp":1580906914858,"user_tz":-330,"elapsed":3584,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":109}},"source":["import os \n","os.chdir(\"/content/drive/My Drive/Image_Resolution/\")\n","os.getcwd()\n","!ls"],"execution_count":0,"outputs":[{"output_type":"stream","text":["butterfly.png\tinputTes_rgb.png  __pycache__\t\t    test.h5\n","crop_train.h5\tout22_rgb.png\t  SRCNN_check915.h5\t    test_rgb.h5\n","face.png\toutTes_rgb.png\t  SRCNN_check_rgb_9-3-5.h5  train_rgb.h5\n","input2_rgb.png\tprepare_data.py   SRCNN_check_rgb_9-5-5.h5\n","input3_rgb.png\tpre_rgb.png\t  SRCNN_check_rgb.h5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Zk6-ZqmZ4bTg","colab_type":"code","outputId":"7f6e7385-1d06-470b-ac1d-55e4d06295b3","executionInfo":{"status":"ok","timestamp":1580906918551,"user_tz":-330,"elapsed":2942,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":83}},"source":["from keras.models import Sequential\n","from keras.layers import Conv2D, Input, BatchNormalization\n","# from keras.layers.advanced_activations import LeakyReLU\n","from keras.callbacks import ModelCheckpoint,Callback\n","from keras.optimizers import SGD, Adam\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","import prepare_data as pd\n","import numpy \n","import math"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"oo6F17dLx1El","colab_type":"code","outputId":"830e5dc9-e536-42a7-df7b-971b7c7a1cb9","executionInfo":{"status":"error","timestamp":1580905751411,"user_tz":-330,"elapsed":3586,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":247}},"source":["\n","%matplotlib inline\n","data_rgb,label_rgb=pd.read_training_data(\"test_rgb.h5\")\n","print(data.shape,label.shape)\n","print(data[0][0][0][0])\n","data_img=data_rgb[5]\n","label_img=label_rgb[5]\n","data=data*255\n","label=label*255\n","fig,axes=plt.subplots(1,2)\n","axes[0].imshow(data_img)\n","axes[1].imshow(label_img)\n"],"execution_count":0,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-122b32daf3b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata_rgb\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel_rgb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_rgb.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_img\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_rgb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"]}]},{"cell_type":"code","metadata":{"id":"jG0RN41fY1pk","colab_type":"code","outputId":"bcf46270-64dc-4d51-81b9-42c8b47fea71","executionInfo":{"status":"ok","timestamp":1580906922225,"user_tz":-330,"elapsed":1218,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["data_Y, label_Y = pd.read_training_data(\"./test.h5\")\n","print(data_Y.shape,label_Y.shape)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(420, 32, 32, 1) (420, 20, 20, 1)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CzafFwFK6wiP","colab_type":"code","colab":{}},"source":["def psnr(target, ref):\n","    mse = numpy.mean( (target - ref) ** 2 )\n","    PIXEL_MAX = 255.0\n","    if mse == 0:\n","        return 100\n","    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"1g-jFwan61pN","colab_type":"code","colab":{}},"source":["def model915():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(32, 32, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=1, nb_col=1, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","def predict_model915():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(None, None, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=1, nb_col=1, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","\n","\n","def model935():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(32, 32, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=3, nb_col=3, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","def predict_model935():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(None, None, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=3, nb_col=3, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","\n","\n","def model955():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(32, 32, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","def predict_model955():\n","    # lrelu = LeakyReLU(alpha=0.1)\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(nb_filter=128, nb_row=9, nb_col=9, init='glorot_uniform',\n","                     activation='relu', border_mode='valid', bias=True, input_shape=(None, None, 1)))\n","    SRCNN.add(Conv2D(nb_filter=64, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='relu', border_mode='same', bias=True))\n","    # SRCNN.add(BatchNormalization())\n","    SRCNN.add(Conv2D(nb_filter=1, nb_row=5, nb_col=5, init='glorot_uniform',\n","                     activation='linear', border_mode='valid', bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","\n","\n","class PsnrHistory915(Callback):\n","    \n","\n","    def on_train_begin(self, logs={}):\n","        self.psnrs = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        \n","        srcnn_model=predict_model915()\n","        weights_file=\"SRCNN_check915.h5\"\n","        srcnn_model.load_weights(weights_file)\n","        avg_psnr=0.0\n","        import cv2\n","        for i in range(0,data_Y.shape[0]):\n","            img=data_Y[i]\n","            Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","            Y[0, :, :, :] = img\n","            pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","            pre[pre[:] > 255] = 255\n","            pre[pre[:] < 0] = 0\n","            img_pre=pre[0,:,:,:]\n","            avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n","\n","        avg_psnr=(avg_psnr/data_Y.shape[0])\n","        \n","        self.psnrs.append((avg_psnr))\n","class PsnrHistory935(Callback):\n","    \n","\n","    def on_train_begin(self, logs={}):\n","        self.psnrs = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        \n","        srcnn_model=predict_model935()\n","        weights_file=\"SRCNN_check935.h5\"\n","        srcnn_model.load_weights(weights_file)\n","        avg_psnr=0.0\n","        import cv2\n","        for i in range(0,data_Y.shape[0]):\n","            img=data_Y[i]\n","            Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","            Y[0, :, :, :] = img\n","            pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","            pre[pre[:] > 255] = 255\n","            pre[pre[:] < 0] = 0\n","            img_pre=pre[0,:,:,:]\n","            avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n","\n","        avg_psnr=(avg_psnr/data_Y.shape[0])\n","        \n","        self.psnrs.append((avg_psnr))\n","class PsnrHistory955(Callback):\n","    \n","\n","    def on_train_begin(self, logs={}):\n","        self.psnrs = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        \n","        srcnn_model=predict_model955()\n","        weights_file=\"SRCNN_check955.h5\"\n","        srcnn_model.load_weights(weights_file)\n","        avg_psnr=0.0\n","        import cv2\n","        for i in range(0,data_Y.shape[0]):\n","            img=data_Y[i]\n","            Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","            Y[0, :, :, :] = img\n","            pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","            pre[pre[:] > 255] = 255\n","            pre[pre[:] < 0] = 0\n","            img_pre=pre[0,:,:,:]\n","            avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n","\n","        avg_psnr=(avg_psnr/data_Y.shape[0])\n","        \n","        self.psnrs.append((avg_psnr))\n","\n","        \n","        # avg_psnr2=0.0\n","        # for i in range(0,data_rgb.shape[0]):\n","        #     img = cv2.cvtColor(data_rgb[i], cv2.COLOR_BGR2YCrCb)\n","        #     Y_img=data_Y[i]\n","        #     Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","        #     Y[0, :, :, :] = Y_img\n","        #     pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","        #     pre[pre[:] > 255] = 255\n","        #     pre[pre[:] < 0] = 0\n","        #     img=img[6: -6, 6: -6,:]\n","        #     img[:,:,0]=pre[0,:,:,0]\n","        #     img=cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n","        #     avg_psnr2=avg_psnr2+psnr(label_rgb[i]*255, img)\n","        \n","        # avg_psnr2=(avg_psnr2/data_rgb.shape[0])\n","        \n","\n","\n","def train915(history):\n","    srcnn_model = model915()\n","    print(srcnn_model.summary())\n","    data, label = pd.read_training_data(\"./crop_train.h5\")\n","    val_data, val_label = pd.read_training_data(\"./test.h5\")\n","\n","    checkpoint = ModelCheckpoint(\"SRCNN_check915.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n","                                 save_weights_only=False, mode='min')\n","    callbacks_list = [checkpoint,history]\n","\n","    srcnn_model.fit(data, label, batch_size=128, validation_data=(val_data, val_label),\n","                    callbacks=callbacks_list, shuffle=True, nb_epoch=200, verbose=0)\n","    # srcnn_model.load_weights(\"m_model_adam.h5\")\n","\n","\n","def train935(history):\n","    srcnn_model = model935()\n","    print(srcnn_model.summary())\n","    data, label = pd.read_training_data(\"./crop_train.h5\")\n","    val_data, val_label = pd.read_training_data(\"./test.h5\")\n","\n","    checkpoint = ModelCheckpoint(\"SRCNN_check935.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n","                                 save_weights_only=False, mode='min')\n","    callbacks_list = [checkpoint,history]\n","\n","    srcnn_model.fit(data, label, batch_size=128, validation_data=(val_data, val_label),\n","                    callbacks=callbacks_list, shuffle=True, nb_epoch=200, verbose=0)\n","    # srcnn_model.load_weights(\"m_model_adam.h5\")\n","\n","\n","def train955(history):\n","    srcnn_model = model955()\n","    print(srcnn_model.summary())\n","    data, label = pd.read_training_data(\"./crop_train.h5\")\n","    val_data, val_label = pd.read_training_data(\"./test.h5\")\n","\n","    checkpoint = ModelCheckpoint(\"SRCNN_check955.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n","                                 save_weights_only=False, mode='min')\n","    callbacks_list = [checkpoint,history]\n","\n","    srcnn_model.fit(data, label, batch_size=128, validation_data=(val_data, val_label),\n","                    callbacks=callbacks_list, shuffle=True, nb_epoch=200, verbose=0)\n","    # srcnn_model.load_weights(\"m_model_adam.h5\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-UKuNV-s69im","colab_type":"code","colab":{}},"source":["def predict(architecture):\n","    if architecture==\"915\":\n","      srcnn_model = predict_model915()\n","    elif architecture==\"935\":\n","      srcnn_model=predict_model935()\n","    else:\n","      srcnn_model=predict_model955()\n","    weight_file=\"SRCNN_check\"+architecture+\".h5\" \n","    srcnn_model.load_weights(weight_file)\n","    IMG_NAME = \"butterfly.png\"\n","    INPUT_NAME = \"Input to \"+architecture+\".png\"\n","    OUTPUT_NAME = \"Output of \"+architecture+\".png\"\n","\n","    import cv2\n","    img = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n","    shape = img.shape\n","    Y_img = cv2.resize(img[:, :, 0], (int(shape[1] / 2), int(shape[0] / 2)), cv2.INTER_CUBIC)\n","    Y_img = cv2.resize(Y_img, (shape[1], shape[0]), cv2.INTER_CUBIC)\n","    print(img.shape,Y_img.shape)\n","    img[:, :, 0] = Y_img\n","    Y_img=img[:,:,0]\n","    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n","    cv2.imwrite(INPUT_NAME, img)\n","\n","    Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","    Y[0, :, :, 0] = Y_img.astype(float) / 255.\n","    pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","    print(pre.shape)\n","    pre[pre[:] > 255] = 255\n","    pre[pre[:] < 0] = 0\n","    pre = pre.astype(numpy.uint8)\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n","    img[6: -6, 6: -6, 0] = pre[0, :, :, 0]\n","    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n","    print(img.shape)\n","    cv2.imwrite(OUTPUT_NAME, img)\n","\n","    # psnr calculation:\n","    im1_rgb = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6, :]\n","    im1_Y = cv2.cvtColor(im1_rgb, cv2.COLOR_BGR2YCrCb)\n","    im2_rgb = cv2.imread(INPUT_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6, :]\n","    im2_Y = cv2.cvtColor(im2_rgb, cv2.COLOR_BGR2YCrCb)\n","    im3_rgb = cv2.imread(OUTPUT_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6, :]\n","    im3_Y = cv2.cvtColor(im3_rgb, cv2.COLOR_BGR2YCrCb)\n","    print(im3_Y.shape,im1_Y.shape)\n","    print(\"bicubic:\")\n","    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im2_Y[:,:,0]),psnr(im1_rgb,im2_rgb)))\n","    print(\"SRCNN:\")\n","    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im3_Y[:,:,0]),psnr(im1_rgb,im3_rgb)))\n","\n","    # srcnn_model = predict_model()\n","    # srcnn_model.load_weights(\"SRCNN_check.h5\")\n","    # avg_psnr=0.0\n","    # for i in range(0,data_Y.shape[0]):\n","    #     img=data_Y[i]\n","    #     Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n","    #     Y[0, :, :, :] = img\n","    #     pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","    #     pre[pre[:] > 255] = 255\n","    #     pre[pre[:] < 0] = 0\n","    #     img_pre=pre[0,:,:,:]\n","    #     avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n","\n","    # avg_psnr=(avg_psnr/data_rgb.shape[0])\n","    # print(avg_psnr)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G0EjckXcwdTG","colab_type":"code","outputId":"ae0a0986-cfc7-40f4-b661-7a5409e43a79","executionInfo":{"status":"ok","timestamp":1580917926263,"user_tz":-330,"elapsed":3479024,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history915=PsnrHistory915()\n","\n","\n","train915(history915)\n","print(history915.psnrs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(32, 32, 1..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","  \"\"\"\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(1, 1), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","  import sys\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","  # Remove the CWD from sys.path while we load stuff.\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:194: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_623\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1867 (Conv2D)         (None, 24, 24, 128)       10496     \n","_________________________________________________________________\n","conv2d_1868 (Conv2D)         (None, 24, 24, 64)        8256      \n","_________________________________________________________________\n","conv2d_1869 (Conv2D)         (None, 20, 20, 1)         1601      \n","=================================================================\n","Total params: 20,353\n","Trainable params: 20,353\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Epoch 00001: val_loss improved from inf to 0.00706, saving model to SRCNN_check915.h5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(1, 1), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 00002: val_loss improved from 0.00706 to 0.00512, saving model to SRCNN_check915.h5\n","\n","Epoch 00003: val_loss improved from 0.00512 to 0.00341, saving model to SRCNN_check915.h5\n","\n","Epoch 00004: val_loss improved from 0.00341 to 0.00265, saving model to SRCNN_check915.h5\n","\n","Epoch 00005: val_loss improved from 0.00265 to 0.00229, saving model to SRCNN_check915.h5\n","\n","Epoch 00006: val_loss improved from 0.00229 to 0.00205, saving model to SRCNN_check915.h5\n","\n","Epoch 00007: val_loss improved from 0.00205 to 0.00187, saving model to SRCNN_check915.h5\n","\n","Epoch 00008: val_loss improved from 0.00187 to 0.00177, saving model to SRCNN_check915.h5\n","\n","Epoch 00009: val_loss improved from 0.00177 to 0.00169, saving model to SRCNN_check915.h5\n","\n","Epoch 00010: val_loss improved from 0.00169 to 0.00163, saving model to SRCNN_check915.h5\n","\n","Epoch 00011: val_loss improved from 0.00163 to 0.00158, saving model to SRCNN_check915.h5\n","\n","Epoch 00012: val_loss improved from 0.00158 to 0.00155, saving model to SRCNN_check915.h5\n","\n","Epoch 00013: val_loss improved from 0.00155 to 0.00152, saving model to SRCNN_check915.h5\n","\n","Epoch 00014: val_loss improved from 0.00152 to 0.00150, saving model to SRCNN_check915.h5\n","\n","Epoch 00015: val_loss improved from 0.00150 to 0.00148, saving model to SRCNN_check915.h5\n","\n","Epoch 00016: val_loss improved from 0.00148 to 0.00148, saving model to SRCNN_check915.h5\n","\n","Epoch 00017: val_loss improved from 0.00148 to 0.00146, saving model to SRCNN_check915.h5\n","\n","Epoch 00018: val_loss improved from 0.00146 to 0.00145, saving model to SRCNN_check915.h5\n","\n","Epoch 00019: val_loss improved from 0.00145 to 0.00144, saving model to SRCNN_check915.h5\n","\n","Epoch 00020: val_loss improved from 0.00144 to 0.00143, saving model to SRCNN_check915.h5\n","\n","Epoch 00021: val_loss improved from 0.00143 to 0.00142, saving model to SRCNN_check915.h5\n","\n","Epoch 00022: val_loss improved from 0.00142 to 0.00142, saving model to SRCNN_check915.h5\n","\n","Epoch 00023: val_loss improved from 0.00142 to 0.00142, saving model to SRCNN_check915.h5\n","\n","Epoch 00024: val_loss improved from 0.00142 to 0.00141, saving model to SRCNN_check915.h5\n","\n","Epoch 00025: val_loss improved from 0.00141 to 0.00140, saving model to SRCNN_check915.h5\n","\n","Epoch 00026: val_loss improved from 0.00140 to 0.00140, saving model to SRCNN_check915.h5\n","\n","Epoch 00027: val_loss did not improve from 0.00140\n","\n","Epoch 00028: val_loss improved from 0.00140 to 0.00140, saving model to SRCNN_check915.h5\n","\n","Epoch 00029: val_loss improved from 0.00140 to 0.00139, saving model to SRCNN_check915.h5\n","\n","Epoch 00030: val_loss improved from 0.00139 to 0.00138, saving model to SRCNN_check915.h5\n","\n","Epoch 00031: val_loss improved from 0.00138 to 0.00138, saving model to SRCNN_check915.h5\n","\n","Epoch 00032: val_loss improved from 0.00138 to 0.00138, saving model to SRCNN_check915.h5\n","\n","Epoch 00033: val_loss improved from 0.00138 to 0.00138, saving model to SRCNN_check915.h5\n","\n","Epoch 00034: val_loss improved from 0.00138 to 0.00138, saving model to SRCNN_check915.h5\n","\n","Epoch 00035: val_loss improved from 0.00138 to 0.00137, saving model to SRCNN_check915.h5\n","\n","Epoch 00036: val_loss improved from 0.00137 to 0.00137, saving model to SRCNN_check915.h5\n","\n","Epoch 00037: val_loss improved from 0.00137 to 0.00137, saving model to SRCNN_check915.h5\n","\n","Epoch 00038: val_loss improved from 0.00137 to 0.00137, saving model to SRCNN_check915.h5\n","\n","Epoch 00039: val_loss improved from 0.00137 to 0.00136, saving model to SRCNN_check915.h5\n","\n","Epoch 00040: val_loss did not improve from 0.00136\n","\n","Epoch 00041: val_loss improved from 0.00136 to 0.00136, saving model to SRCNN_check915.h5\n","\n","Epoch 00042: val_loss improved from 0.00136 to 0.00136, saving model to SRCNN_check915.h5\n","\n","Epoch 00043: val_loss improved from 0.00136 to 0.00135, saving model to SRCNN_check915.h5\n","\n","Epoch 00044: val_loss did not improve from 0.00135\n","\n","Epoch 00045: val_loss did not improve from 0.00135\n","\n","Epoch 00046: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check915.h5\n","\n","Epoch 00047: val_loss did not improve from 0.00135\n","\n","Epoch 00048: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check915.h5\n","\n","Epoch 00049: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check915.h5\n","\n","Epoch 00050: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check915.h5\n","\n","Epoch 00051: val_loss improved from 0.00135 to 0.00134, saving model to SRCNN_check915.h5\n","\n","Epoch 00052: val_loss did not improve from 0.00134\n","\n","Epoch 00053: val_loss did not improve from 0.00134\n","\n","Epoch 00054: val_loss did not improve from 0.00134\n","\n","Epoch 00055: val_loss did not improve from 0.00134\n","\n","Epoch 00056: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check915.h5\n","\n","Epoch 00057: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check915.h5\n","\n","Epoch 00058: val_loss did not improve from 0.00134\n","\n","Epoch 00059: val_loss did not improve from 0.00134\n","\n","Epoch 00060: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check915.h5\n","\n","Epoch 00061: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check915.h5\n","\n","Epoch 00062: val_loss improved from 0.00134 to 0.00133, saving model to SRCNN_check915.h5\n","\n","Epoch 00063: val_loss did not improve from 0.00133\n","\n","Epoch 00064: val_loss improved from 0.00133 to 0.00133, saving model to SRCNN_check915.h5\n","\n","Epoch 00065: val_loss improved from 0.00133 to 0.00133, saving model to SRCNN_check915.h5\n","\n","Epoch 00066: val_loss did not improve from 0.00133\n","\n","Epoch 00067: val_loss did not improve from 0.00133\n","\n","Epoch 00068: val_loss improved from 0.00133 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00069: val_loss did not improve from 0.00132\n","\n","Epoch 00070: val_loss did not improve from 0.00132\n","\n","Epoch 00071: val_loss did not improve from 0.00132\n","\n","Epoch 00072: val_loss did not improve from 0.00132\n","\n","Epoch 00073: val_loss did not improve from 0.00132\n","\n","Epoch 00074: val_loss did not improve from 0.00132\n","\n","Epoch 00075: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00076: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00077: val_loss did not improve from 0.00132\n","\n","Epoch 00078: val_loss did not improve from 0.00132\n","\n","Epoch 00079: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00080: val_loss did not improve from 0.00132\n","\n","Epoch 00081: val_loss did not improve from 0.00132\n","\n","Epoch 00082: val_loss did not improve from 0.00132\n","\n","Epoch 00083: val_loss did not improve from 0.00132\n","\n","Epoch 00084: val_loss did not improve from 0.00132\n","\n","Epoch 00085: val_loss did not improve from 0.00132\n","\n","Epoch 00086: val_loss did not improve from 0.00132\n","\n","Epoch 00087: val_loss did not improve from 0.00132\n","\n","Epoch 00088: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00089: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check915.h5\n","\n","Epoch 00090: val_loss did not improve from 0.00132\n","\n","Epoch 00091: val_loss did not improve from 0.00132\n","\n","Epoch 00092: val_loss did not improve from 0.00132\n","\n","Epoch 00093: val_loss did not improve from 0.00132\n","\n","Epoch 00094: val_loss improved from 0.00132 to 0.00131, saving model to SRCNN_check915.h5\n","\n","Epoch 00095: val_loss did not improve from 0.00131\n","\n","Epoch 00096: val_loss did not improve from 0.00131\n","\n","Epoch 00097: val_loss did not improve from 0.00131\n","\n","Epoch 00098: val_loss did not improve from 0.00131\n","\n","Epoch 00099: val_loss did not improve from 0.00131\n","\n","Epoch 00100: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check915.h5\n","\n","Epoch 00101: val_loss did not improve from 0.00131\n","\n","Epoch 00102: val_loss did not improve from 0.00131\n","\n","Epoch 00103: val_loss did not improve from 0.00131\n","\n","Epoch 00104: val_loss did not improve from 0.00131\n","\n","Epoch 00105: val_loss did not improve from 0.00131\n","\n","Epoch 00106: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check915.h5\n","\n","Epoch 00107: val_loss did not improve from 0.00131\n","\n","Epoch 00108: val_loss did not improve from 0.00131\n","\n","Epoch 00109: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check915.h5\n","\n","Epoch 00110: val_loss did not improve from 0.00131\n","\n","Epoch 00111: val_loss improved from 0.00131 to 0.00130, saving model to SRCNN_check915.h5\n","\n","Epoch 00112: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check915.h5\n","\n","Epoch 00113: val_loss did not improve from 0.00130\n","\n","Epoch 00114: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check915.h5\n","\n","Epoch 00115: val_loss did not improve from 0.00130\n","\n","Epoch 00116: val_loss did not improve from 0.00130\n","\n","Epoch 00117: val_loss did not improve from 0.00130\n","\n","Epoch 00118: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check915.h5\n","\n","Epoch 00119: val_loss did not improve from 0.00130\n","\n","Epoch 00120: val_loss did not improve from 0.00130\n","\n","Epoch 00121: val_loss did not improve from 0.00130\n","\n","Epoch 00122: val_loss did not improve from 0.00130\n","\n","Epoch 00123: val_loss did not improve from 0.00130\n","\n","Epoch 00124: val_loss did not improve from 0.00130\n","\n","Epoch 00125: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check915.h5\n","\n","Epoch 00126: val_loss did not improve from 0.00130\n","\n","Epoch 00127: val_loss did not improve from 0.00130\n","\n","Epoch 00128: val_loss did not improve from 0.00130\n","\n","Epoch 00129: val_loss did not improve from 0.00130\n","\n","Epoch 00130: val_loss did not improve from 0.00130\n","\n","Epoch 00131: val_loss did not improve from 0.00130\n","\n","Epoch 00132: val_loss did not improve from 0.00130\n","\n","Epoch 00133: val_loss did not improve from 0.00130\n","\n","Epoch 00134: val_loss did not improve from 0.00130\n","\n","Epoch 00135: val_loss did not improve from 0.00130\n","\n","Epoch 00136: val_loss did not improve from 0.00130\n","\n","Epoch 00137: val_loss did not improve from 0.00130\n","\n","Epoch 00138: val_loss did not improve from 0.00130\n","\n","Epoch 00139: val_loss did not improve from 0.00130\n","\n","Epoch 00140: val_loss did not improve from 0.00130\n","\n","Epoch 00141: val_loss did not improve from 0.00130\n","\n","Epoch 00142: val_loss did not improve from 0.00130\n","\n","Epoch 00143: val_loss did not improve from 0.00130\n","\n","Epoch 00144: val_loss did not improve from 0.00130\n","\n","Epoch 00145: val_loss improved from 0.00130 to 0.00129, saving model to SRCNN_check915.h5\n","\n","Epoch 00146: val_loss did not improve from 0.00129\n","\n","Epoch 00147: val_loss did not improve from 0.00129\n","\n","Epoch 00148: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check915.h5\n","\n","Epoch 00149: val_loss did not improve from 0.00129\n","\n","Epoch 00150: val_loss did not improve from 0.00129\n","\n","Epoch 00151: val_loss did not improve from 0.00129\n","\n","Epoch 00152: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check915.h5\n","\n","Epoch 00153: val_loss did not improve from 0.00129\n","\n","Epoch 00154: val_loss did not improve from 0.00129\n","\n","Epoch 00155: val_loss did not improve from 0.00129\n","\n","Epoch 00156: val_loss did not improve from 0.00129\n","\n","Epoch 00157: val_loss did not improve from 0.00129\n","\n","Epoch 00158: val_loss did not improve from 0.00129\n","\n","Epoch 00159: val_loss did not improve from 0.00129\n","\n","Epoch 00160: val_loss did not improve from 0.00129\n","\n","Epoch 00161: val_loss did not improve from 0.00129\n","\n","Epoch 00162: val_loss did not improve from 0.00129\n","\n","Epoch 00163: val_loss did not improve from 0.00129\n","\n","Epoch 00164: val_loss did not improve from 0.00129\n","\n","Epoch 00165: val_loss did not improve from 0.00129\n","\n","Epoch 00166: val_loss did not improve from 0.00129\n","\n","Epoch 00167: val_loss did not improve from 0.00129\n","\n","Epoch 00168: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check915.h5\n","\n","Epoch 00169: val_loss did not improve from 0.00129\n","\n","Epoch 00170: val_loss did not improve from 0.00129\n","\n","Epoch 00171: val_loss did not improve from 0.00129\n","\n","Epoch 00172: val_loss did not improve from 0.00129\n","\n","Epoch 00173: val_loss did not improve from 0.00129\n","\n","Epoch 00174: val_loss did not improve from 0.00129\n","\n","Epoch 00175: val_loss did not improve from 0.00129\n","\n","Epoch 00176: val_loss improved from 0.00129 to 0.00128, saving model to SRCNN_check915.h5\n","\n","Epoch 00177: val_loss did not improve from 0.00128\n","\n","Epoch 00178: val_loss did not improve from 0.00128\n","\n","Epoch 00179: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check915.h5\n","\n","Epoch 00180: val_loss did not improve from 0.00128\n","\n","Epoch 00181: val_loss did not improve from 0.00128\n","\n","Epoch 00182: val_loss did not improve from 0.00128\n","\n","Epoch 00183: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check915.h5\n","\n","Epoch 00184: val_loss did not improve from 0.00128\n","\n","Epoch 00185: val_loss did not improve from 0.00128\n","\n","Epoch 00186: val_loss did not improve from 0.00128\n","\n","Epoch 00187: val_loss did not improve from 0.00128\n","\n","Epoch 00188: val_loss did not improve from 0.00128\n","\n","Epoch 00189: val_loss did not improve from 0.00128\n","\n","Epoch 00190: val_loss did not improve from 0.00128\n","\n","Epoch 00191: val_loss did not improve from 0.00128\n","\n","Epoch 00192: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check915.h5\n","\n","Epoch 00193: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check915.h5\n","\n","Epoch 00194: val_loss did not improve from 0.00128\n","\n","Epoch 00195: val_loss did not improve from 0.00128\n","\n","Epoch 00196: val_loss did not improve from 0.00128\n","\n","Epoch 00197: val_loss did not improve from 0.00128\n","\n","Epoch 00198: val_loss did not improve from 0.00128\n","\n","Epoch 00199: val_loss did not improve from 0.00128\n","\n","Epoch 00200: val_loss did not improve from 0.00128\n","[24.901163980047308, 27.14996335082261, 29.31518275582092, 30.26427117171544, 30.72082366120238, 31.63046279068081, 31.905263068208345, 31.797475804025144, 32.416181873782826, 32.32096241922825, 32.234495377450926, 32.733744398259844, 32.85682420269466, 32.60993946734683, 32.86330896082099, 32.6775924662894, 33.01276849909307, 33.08774420755875, 33.11024436356159, 33.1296804408298, 32.78060117656943, 33.113014690848296, 32.59916150281508, 33.010407546497376, 33.1567798246579, 33.17604653660475, 33.17604653660475, 32.97399034958605, 33.13341865286254, 33.268503945504634, 33.17892851994444, 33.18149077467286, 32.827537973230285, 33.03300578752787, 33.141424195844166, 33.23035583122399, 33.05014760658971, 33.346116076837156, 33.226690947259385, 33.226690947259385, 33.028851594850096, 32.93756472004963, 32.97155174874347, 32.97155174874347, 32.97155174874347, 33.354290924995325, 33.354290924995325, 32.94143272153669, 33.291315626642685, 33.192483653404345, 33.31765020009229, 33.31765020009229, 33.31765020009229, 33.31765020009229, 33.31765020009229, 33.29927485696435, 33.121296348656436, 33.121296348656436, 33.121296348656436, 33.42475429268305, 33.412047498826205, 33.259176083470926, 33.259176083470926, 33.301322993408604, 33.39693066046763, 33.39693066046763, 33.39693066046763, 33.41971448328911, 33.41971448328911, 33.41971448328911, 33.41971448328911, 33.41971448328911, 33.41971448328911, 33.41971448328911, 33.328391748915074, 33.461627252402266, 33.461627252402266, 33.461627252402266, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.4525870130048, 33.158710681779596, 32.98537255738279, 32.98537255738279, 32.98537255738279, 32.98537255738279, 32.98537255738279, 33.34618413118146, 33.34618413118146, 33.34618413118146, 33.34618413118146, 33.34618413118146, 33.34618413118146, 33.3166890403428, 33.3166890403428, 33.3166890403428, 33.3166890403428, 33.3166890403428, 33.3166890403428, 33.117060448042594, 33.117060448042594, 33.117060448042594, 33.38422834305649, 33.38422834305649, 33.351068379029286, 33.26186827805952, 33.26186827805952, 33.33502359159493, 33.33502359159493, 33.33502359159493, 33.33502359159493, 33.312694025841274, 33.312694025841274, 33.312694025841274, 33.312694025841274, 33.312694025841274, 33.312694025841274, 33.312694025841274, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.104912610883794, 33.338529120033485, 33.338529120033485, 33.338529120033485, 33.4757017292513, 33.4757017292513, 33.4757017292513, 33.4757017292513, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.316281839165036, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.34887492526384, 33.05453195787809, 33.05453195787809, 33.05453195787809, 33.284508678839906, 33.284508678839906, 33.284508678839906, 33.284508678839906, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.45713022502861, 33.50233828230343, 33.52515818664586, 33.52515818664586, 33.52515818664586, 33.52515818664586, 33.52515818664586, 33.52515818664586, 33.52515818664586, 33.52515818664586]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"BAJr8VdLaqjC","colab_type":"code","outputId":"b512c671-4630-4305-b963-503088f13b6c","executionInfo":{"status":"ok","timestamp":1580909753163,"user_tz":-330,"elapsed":1466836,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history935=PsnrHistory935()\n","\n","\n","train935(history935)\n","print(history935.psnrs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(32, 32, 1..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:35: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(3, 3), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:209: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_207\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_619 (Conv2D)          (None, 24, 24, 128)       10496     \n","_________________________________________________________________\n","conv2d_620 (Conv2D)          (None, 24, 24, 64)        73792     \n","_________________________________________________________________\n","conv2d_621 (Conv2D)          (None, 20, 20, 1)         1601      \n","=================================================================\n","Total params: 85,889\n","Trainable params: 85,889\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Epoch 00001: val_loss improved from inf to 0.00603, saving model to SRCNN_check935.h5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(3, 3), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 00002: val_loss improved from 0.00603 to 0.00375, saving model to SRCNN_check935.h5\n","\n","Epoch 00003: val_loss improved from 0.00375 to 0.00266, saving model to SRCNN_check935.h5\n","\n","Epoch 00004: val_loss improved from 0.00266 to 0.00219, saving model to SRCNN_check935.h5\n","\n","Epoch 00005: val_loss improved from 0.00219 to 0.00192, saving model to SRCNN_check935.h5\n","\n","Epoch 00006: val_loss improved from 0.00192 to 0.00177, saving model to SRCNN_check935.h5\n","\n","Epoch 00007: val_loss improved from 0.00177 to 0.00167, saving model to SRCNN_check935.h5\n","\n","Epoch 00008: val_loss improved from 0.00167 to 0.00158, saving model to SRCNN_check935.h5\n","\n","Epoch 00009: val_loss improved from 0.00158 to 0.00152, saving model to SRCNN_check935.h5\n","\n","Epoch 00010: val_loss improved from 0.00152 to 0.00147, saving model to SRCNN_check935.h5\n","\n","Epoch 00011: val_loss improved from 0.00147 to 0.00145, saving model to SRCNN_check935.h5\n","\n","Epoch 00012: val_loss did not improve from 0.00145\n","\n","Epoch 00013: val_loss improved from 0.00145 to 0.00141, saving model to SRCNN_check935.h5\n","\n","Epoch 00014: val_loss improved from 0.00141 to 0.00141, saving model to SRCNN_check935.h5\n","\n","Epoch 00015: val_loss improved from 0.00141 to 0.00139, saving model to SRCNN_check935.h5\n","\n","Epoch 00016: val_loss improved from 0.00139 to 0.00138, saving model to SRCNN_check935.h5\n","\n","Epoch 00017: val_loss improved from 0.00138 to 0.00137, saving model to SRCNN_check935.h5\n","\n","Epoch 00018: val_loss improved from 0.00137 to 0.00137, saving model to SRCNN_check935.h5\n","\n","Epoch 00019: val_loss improved from 0.00137 to 0.00136, saving model to SRCNN_check935.h5\n","\n","Epoch 00020: val_loss improved from 0.00136 to 0.00136, saving model to SRCNN_check935.h5\n","\n","Epoch 00021: val_loss did not improve from 0.00136\n","\n","Epoch 00022: val_loss improved from 0.00136 to 0.00136, saving model to SRCNN_check935.h5\n","\n","Epoch 00023: val_loss did not improve from 0.00136\n","\n","Epoch 00024: val_loss improved from 0.00136 to 0.00135, saving model to SRCNN_check935.h5\n","\n","Epoch 00025: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check935.h5\n","\n","Epoch 00026: val_loss improved from 0.00135 to 0.00134, saving model to SRCNN_check935.h5\n","\n","Epoch 00027: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check935.h5\n","\n","Epoch 00028: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check935.h5\n","\n","Epoch 00029: val_loss improved from 0.00134 to 0.00133, saving model to SRCNN_check935.h5\n","\n","Epoch 00030: val_loss did not improve from 0.00133\n","\n","Epoch 00031: val_loss did not improve from 0.00133\n","\n","Epoch 00032: val_loss did not improve from 0.00133\n","\n","Epoch 00033: val_loss did not improve from 0.00133\n","\n","Epoch 00034: val_loss did not improve from 0.00133\n","\n","Epoch 00035: val_loss did not improve from 0.00133\n","\n","Epoch 00036: val_loss did not improve from 0.00133\n","\n","Epoch 00037: val_loss did not improve from 0.00133\n","\n","Epoch 00038: val_loss did not improve from 0.00133\n","\n","Epoch 00039: val_loss improved from 0.00133 to 0.00133, saving model to SRCNN_check935.h5\n","\n","Epoch 00040: val_loss improved from 0.00133 to 0.00132, saving model to SRCNN_check935.h5\n","\n","Epoch 00041: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check935.h5\n","\n","Epoch 00042: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check935.h5\n","\n","Epoch 00043: val_loss did not improve from 0.00132\n","\n","Epoch 00044: val_loss improved from 0.00132 to 0.00131, saving model to SRCNN_check935.h5\n","\n","Epoch 00045: val_loss did not improve from 0.00131\n","\n","Epoch 00046: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check935.h5\n","\n","Epoch 00047: val_loss did not improve from 0.00131\n","\n","Epoch 00048: val_loss did not improve from 0.00131\n","\n","Epoch 00049: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check935.h5\n","\n","Epoch 00050: val_loss did not improve from 0.00131\n","\n","Epoch 00051: val_loss improved from 0.00131 to 0.00130, saving model to SRCNN_check935.h5\n","\n","Epoch 00052: val_loss did not improve from 0.00130\n","\n","Epoch 00053: val_loss did not improve from 0.00130\n","\n","Epoch 00054: val_loss did not improve from 0.00130\n","\n","Epoch 00055: val_loss did not improve from 0.00130\n","\n","Epoch 00056: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check935.h5\n","\n","Epoch 00057: val_loss did not improve from 0.00130\n","\n","Epoch 00058: val_loss did not improve from 0.00130\n","\n","Epoch 00059: val_loss did not improve from 0.00130\n","\n","Epoch 00060: val_loss did not improve from 0.00130\n","\n","Epoch 00061: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check935.h5\n","\n","Epoch 00062: val_loss did not improve from 0.00130\n","\n","Epoch 00063: val_loss did not improve from 0.00130\n","\n","Epoch 00064: val_loss did not improve from 0.00130\n","\n","Epoch 00065: val_loss did not improve from 0.00130\n","\n","Epoch 00066: val_loss did not improve from 0.00130\n","\n","Epoch 00067: val_loss did not improve from 0.00130\n","\n","Epoch 00068: val_loss did not improve from 0.00130\n","\n","Epoch 00069: val_loss improved from 0.00130 to 0.00129, saving model to SRCNN_check935.h5\n","\n","Epoch 00070: val_loss did not improve from 0.00129\n","\n","Epoch 00071: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check935.h5\n","\n","Epoch 00072: val_loss did not improve from 0.00129\n","\n","Epoch 00073: val_loss did not improve from 0.00129\n","\n","Epoch 00074: val_loss did not improve from 0.00129\n","\n","Epoch 00075: val_loss did not improve from 0.00129\n","\n","Epoch 00076: val_loss did not improve from 0.00129\n","\n","Epoch 00077: val_loss did not improve from 0.00129\n","\n","Epoch 00078: val_loss did not improve from 0.00129\n","\n","Epoch 00079: val_loss did not improve from 0.00129\n","\n","Epoch 00080: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check935.h5\n","\n","Epoch 00081: val_loss did not improve from 0.00129\n","\n","Epoch 00082: val_loss did not improve from 0.00129\n","\n","Epoch 00083: val_loss did not improve from 0.00129\n","\n","Epoch 00084: val_loss did not improve from 0.00129\n","\n","Epoch 00085: val_loss did not improve from 0.00129\n","\n","Epoch 00086: val_loss did not improve from 0.00129\n","\n","Epoch 00087: val_loss did not improve from 0.00129\n","\n","Epoch 00088: val_loss did not improve from 0.00129\n","\n","Epoch 00089: val_loss did not improve from 0.00129\n","\n","Epoch 00090: val_loss improved from 0.00129 to 0.00128, saving model to SRCNN_check935.h5\n","\n","Epoch 00091: val_loss did not improve from 0.00128\n","\n","Epoch 00092: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check935.h5\n","\n","Epoch 00093: val_loss did not improve from 0.00128\n","\n","Epoch 00094: val_loss did not improve from 0.00128\n","\n","Epoch 00095: val_loss did not improve from 0.00128\n","\n","Epoch 00096: val_loss did not improve from 0.00128\n","\n","Epoch 00097: val_loss did not improve from 0.00128\n","\n","Epoch 00098: val_loss did not improve from 0.00128\n","\n","Epoch 00099: val_loss did not improve from 0.00128\n","\n","Epoch 00100: val_loss did not improve from 0.00128\n","\n","Epoch 00101: val_loss did not improve from 0.00128\n","\n","Epoch 00102: val_loss did not improve from 0.00128\n","\n","Epoch 00103: val_loss did not improve from 0.00128\n","\n","Epoch 00104: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check935.h5\n","\n","Epoch 00105: val_loss improved from 0.00128 to 0.00127, saving model to SRCNN_check935.h5\n","\n","Epoch 00106: val_loss did not improve from 0.00127\n","\n","Epoch 00107: val_loss did not improve from 0.00127\n","\n","Epoch 00108: val_loss did not improve from 0.00127\n","\n","Epoch 00109: val_loss did not improve from 0.00127\n","\n","Epoch 00110: val_loss did not improve from 0.00127\n","\n","Epoch 00111: val_loss did not improve from 0.00127\n","\n","Epoch 00112: val_loss did not improve from 0.00127\n","\n","Epoch 00113: val_loss did not improve from 0.00127\n","\n","Epoch 00114: val_loss did not improve from 0.00127\n","\n","Epoch 00115: val_loss did not improve from 0.00127\n","\n","Epoch 00116: val_loss did not improve from 0.00127\n","\n","Epoch 00117: val_loss did not improve from 0.00127\n","\n","Epoch 00118: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check935.h5\n","\n","Epoch 00119: val_loss did not improve from 0.00127\n","\n","Epoch 00120: val_loss did not improve from 0.00127\n","\n","Epoch 00121: val_loss did not improve from 0.00127\n","\n","Epoch 00122: val_loss did not improve from 0.00127\n","\n","Epoch 00123: val_loss did not improve from 0.00127\n","\n","Epoch 00124: val_loss did not improve from 0.00127\n","\n","Epoch 00125: val_loss did not improve from 0.00127\n","\n","Epoch 00126: val_loss did not improve from 0.00127\n","\n","Epoch 00127: val_loss did not improve from 0.00127\n","\n","Epoch 00128: val_loss did not improve from 0.00127\n","\n","Epoch 00129: val_loss did not improve from 0.00127\n","\n","Epoch 00130: val_loss did not improve from 0.00127\n","\n","Epoch 00131: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check935.h5\n","\n","Epoch 00132: val_loss did not improve from 0.00127\n","\n","Epoch 00133: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check935.h5\n","\n","Epoch 00134: val_loss did not improve from 0.00127\n","\n","Epoch 00135: val_loss improved from 0.00127 to 0.00126, saving model to SRCNN_check935.h5\n","\n","Epoch 00136: val_loss did not improve from 0.00126\n","\n","Epoch 00137: val_loss did not improve from 0.00126\n","\n","Epoch 00138: val_loss did not improve from 0.00126\n","\n","Epoch 00139: val_loss did not improve from 0.00126\n","\n","Epoch 00140: val_loss did not improve from 0.00126\n","\n","Epoch 00141: val_loss did not improve from 0.00126\n","\n","Epoch 00142: val_loss did not improve from 0.00126\n","\n","Epoch 00143: val_loss did not improve from 0.00126\n","\n","Epoch 00144: val_loss did not improve from 0.00126\n","\n","Epoch 00145: val_loss did not improve from 0.00126\n","\n","Epoch 00146: val_loss did not improve from 0.00126\n","\n","Epoch 00147: val_loss improved from 0.00126 to 0.00126, saving model to SRCNN_check935.h5\n","\n","Epoch 00148: val_loss did not improve from 0.00126\n","\n","Epoch 00149: val_loss did not improve from 0.00126\n","\n","Epoch 00150: val_loss did not improve from 0.00126\n","\n","Epoch 00151: val_loss did not improve from 0.00126\n","\n","Epoch 00152: val_loss did not improve from 0.00126\n","\n","Epoch 00153: val_loss did not improve from 0.00126\n","\n","Epoch 00154: val_loss improved from 0.00126 to 0.00126, saving model to SRCNN_check935.h5\n","\n","Epoch 00155: val_loss did not improve from 0.00126\n","\n","Epoch 00156: val_loss did not improve from 0.00126\n","\n","Epoch 00157: val_loss did not improve from 0.00126\n","\n","Epoch 00158: val_loss did not improve from 0.00126\n","\n","Epoch 00159: val_loss did not improve from 0.00126\n","\n","Epoch 00160: val_loss did not improve from 0.00126\n","\n","Epoch 00161: val_loss did not improve from 0.00126\n","\n","Epoch 00162: val_loss did not improve from 0.00126\n","\n","Epoch 00163: val_loss did not improve from 0.00126\n","\n","Epoch 00164: val_loss did not improve from 0.00126\n","\n","Epoch 00165: val_loss did not improve from 0.00126\n","\n","Epoch 00166: val_loss did not improve from 0.00126\n","\n","Epoch 00167: val_loss did not improve from 0.00126\n","\n","Epoch 00168: val_loss did not improve from 0.00126\n","\n","Epoch 00169: val_loss did not improve from 0.00126\n","\n","Epoch 00170: val_loss did not improve from 0.00126\n","\n","Epoch 00171: val_loss did not improve from 0.00126\n","\n","Epoch 00172: val_loss did not improve from 0.00126\n","\n","Epoch 00173: val_loss improved from 0.00126 to 0.00126, saving model to SRCNN_check935.h5\n","\n","Epoch 00174: val_loss did not improve from 0.00126\n","\n","Epoch 00175: val_loss did not improve from 0.00126\n","\n","Epoch 00176: val_loss did not improve from 0.00126\n","\n","Epoch 00177: val_loss did not improve from 0.00126\n","\n","Epoch 00178: val_loss did not improve from 0.00126\n","\n","Epoch 00179: val_loss did not improve from 0.00126\n","\n","Epoch 00180: val_loss did not improve from 0.00126\n","\n","Epoch 00181: val_loss did not improve from 0.00126\n","\n","Epoch 00182: val_loss did not improve from 0.00126\n","\n","Epoch 00183: val_loss did not improve from 0.00126\n","\n","Epoch 00184: val_loss did not improve from 0.00126\n","\n","Epoch 00185: val_loss did not improve from 0.00126\n","\n","Epoch 00186: val_loss did not improve from 0.00126\n","\n","Epoch 00187: val_loss did not improve from 0.00126\n","\n","Epoch 00188: val_loss did not improve from 0.00126\n","\n","Epoch 00189: val_loss did not improve from 0.00126\n","\n","Epoch 00190: val_loss did not improve from 0.00126\n","\n","Epoch 00191: val_loss did not improve from 0.00126\n","\n","Epoch 00192: val_loss did not improve from 0.00126\n","\n","Epoch 00193: val_loss did not improve from 0.00126\n","\n","Epoch 00194: val_loss did not improve from 0.00126\n","\n","Epoch 00195: val_loss improved from 0.00126 to 0.00126, saving model to SRCNN_check935.h5\n","\n","Epoch 00196: val_loss did not improve from 0.00126\n","\n","Epoch 00197: val_loss did not improve from 0.00126\n","\n","Epoch 00198: val_loss did not improve from 0.00126\n","\n","Epoch 00199: val_loss did not improve from 0.00126\n","\n","Epoch 00200: val_loss did not improve from 0.00126\n","[25.09427695111339, 28.22092566500807, 29.76666444135584, 30.19559485877884, 31.56154906301195, 31.936467267927934, 31.71730220340454, 32.46787679556609, 32.45315982882129, 32.57004926560764, 32.721886091928525, 32.721886091928525, 33.06296682369763, 32.29678812655256, 32.815697161179905, 33.05824269958163, 33.08767447802629, 33.040680787030325, 32.99864837507955, 33.054315847251246, 33.054315847251246, 32.90165169574671, 32.90165169574671, 33.06345923319717, 33.02585227533511, 32.914509545248436, 33.09512743630711, 33.07679881873679, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.09174910643072, 33.02219298386508, 33.138762550522635, 33.31045141744219, 33.31045141744219, 33.2649505770164, 33.2649505770164, 33.020135543376526, 33.020135543376526, 33.020135543376526, 33.04417826459989, 33.04417826459989, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 32.97973453436559, 32.97973453436559, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.10154092759482, 33.10154092759482, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.07138066590796, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.31905341213508, 33.31905341213508, 33.48243159846404, 33.48243159846404, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"cSiTc-iparFv","colab_type":"code","outputId":"4de227ad-315e-4b86-92c6-befaac878b0d","executionInfo":{"status":"ok","timestamp":1580912694990,"user_tz":-330,"elapsed":2664475,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["history955=PsnrHistory955()\n","\n","\n","train955(history955)\n","print(history955.psnrs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:61: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(32, 32, 1..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:63: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(5, 5), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:66: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:224: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"],"name":"stderr"},{"output_type":"stream","text":["Model: \"sequential_411\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_1231 (Conv2D)         (None, 24, 24, 128)       10496     \n","_________________________________________________________________\n","conv2d_1232 (Conv2D)         (None, 24, 24, 64)        204864    \n","_________________________________________________________________\n","conv2d_1233 (Conv2D)         (None, 20, 20, 1)         1601      \n","=================================================================\n","Total params: 216,961\n","Trainable params: 216,961\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Epoch 00001: val_loss improved from inf to 0.00454, saving model to SRCNN_check955.h5\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(5, 5), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["\n","Epoch 00002: val_loss improved from 0.00454 to 0.00277, saving model to SRCNN_check955.h5\n","\n","Epoch 00003: val_loss improved from 0.00277 to 0.00205, saving model to SRCNN_check955.h5\n","\n","Epoch 00004: val_loss improved from 0.00205 to 0.00184, saving model to SRCNN_check955.h5\n","\n","Epoch 00005: val_loss improved from 0.00184 to 0.00163, saving model to SRCNN_check955.h5\n","\n","Epoch 00006: val_loss improved from 0.00163 to 0.00154, saving model to SRCNN_check955.h5\n","\n","Epoch 00007: val_loss improved from 0.00154 to 0.00148, saving model to SRCNN_check955.h5\n","\n","Epoch 00008: val_loss improved from 0.00148 to 0.00145, saving model to SRCNN_check955.h5\n","\n","Epoch 00009: val_loss improved from 0.00145 to 0.00143, saving model to SRCNN_check955.h5\n","\n","Epoch 00010: val_loss improved from 0.00143 to 0.00141, saving model to SRCNN_check955.h5\n","\n","Epoch 00011: val_loss improved from 0.00141 to 0.00141, saving model to SRCNN_check955.h5\n","\n","Epoch 00012: val_loss improved from 0.00141 to 0.00139, saving model to SRCNN_check955.h5\n","\n","Epoch 00013: val_loss did not improve from 0.00139\n","\n","Epoch 00014: val_loss improved from 0.00139 to 0.00138, saving model to SRCNN_check955.h5\n","\n","Epoch 00015: val_loss improved from 0.00138 to 0.00137, saving model to SRCNN_check955.h5\n","\n","Epoch 00016: val_loss did not improve from 0.00137\n","\n","Epoch 00017: val_loss improved from 0.00137 to 0.00137, saving model to SRCNN_check955.h5\n","\n","Epoch 00018: val_loss improved from 0.00137 to 0.00136, saving model to SRCNN_check955.h5\n","\n","Epoch 00019: val_loss improved from 0.00136 to 0.00136, saving model to SRCNN_check955.h5\n","\n","Epoch 00020: val_loss improved from 0.00136 to 0.00135, saving model to SRCNN_check955.h5\n","\n","Epoch 00021: val_loss did not improve from 0.00135\n","\n","Epoch 00022: val_loss did not improve from 0.00135\n","\n","Epoch 00023: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check955.h5\n","\n","Epoch 00024: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check955.h5\n","\n","Epoch 00025: val_loss did not improve from 0.00135\n","\n","Epoch 00026: val_loss improved from 0.00135 to 0.00135, saving model to SRCNN_check955.h5\n","\n","Epoch 00027: val_loss did not improve from 0.00135\n","\n","Epoch 00028: val_loss did not improve from 0.00135\n","\n","Epoch 00029: val_loss improved from 0.00135 to 0.00134, saving model to SRCNN_check955.h5\n","\n","Epoch 00030: val_loss improved from 0.00134 to 0.00134, saving model to SRCNN_check955.h5\n","\n","Epoch 00031: val_loss did not improve from 0.00134\n","\n","Epoch 00032: val_loss did not improve from 0.00134\n","\n","Epoch 00033: val_loss did not improve from 0.00134\n","\n","Epoch 00034: val_loss improved from 0.00134 to 0.00133, saving model to SRCNN_check955.h5\n","\n","Epoch 00035: val_loss improved from 0.00133 to 0.00133, saving model to SRCNN_check955.h5\n","\n","Epoch 00036: val_loss did not improve from 0.00133\n","\n","Epoch 00037: val_loss improved from 0.00133 to 0.00133, saving model to SRCNN_check955.h5\n","\n","Epoch 00038: val_loss did not improve from 0.00133\n","\n","Epoch 00039: val_loss did not improve from 0.00133\n","\n","Epoch 00040: val_loss improved from 0.00133 to 0.00132, saving model to SRCNN_check955.h5\n","\n","Epoch 00041: val_loss improved from 0.00132 to 0.00132, saving model to SRCNN_check955.h5\n","\n","Epoch 00042: val_loss did not improve from 0.00132\n","\n","Epoch 00043: val_loss did not improve from 0.00132\n","\n","Epoch 00044: val_loss did not improve from 0.00132\n","\n","Epoch 00045: val_loss did not improve from 0.00132\n","\n","Epoch 00046: val_loss did not improve from 0.00132\n","\n","Epoch 00047: val_loss improved from 0.00132 to 0.00131, saving model to SRCNN_check955.h5\n","\n","Epoch 00048: val_loss did not improve from 0.00131\n","\n","Epoch 00049: val_loss did not improve from 0.00131\n","\n","Epoch 00050: val_loss did not improve from 0.00131\n","\n","Epoch 00051: val_loss did not improve from 0.00131\n","\n","Epoch 00052: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check955.h5\n","\n","Epoch 00053: val_loss improved from 0.00131 to 0.00131, saving model to SRCNN_check955.h5\n","\n","Epoch 00054: val_loss did not improve from 0.00131\n","\n","Epoch 00055: val_loss did not improve from 0.00131\n","\n","Epoch 00056: val_loss did not improve from 0.00131\n","\n","Epoch 00057: val_loss did not improve from 0.00131\n","\n","Epoch 00058: val_loss did not improve from 0.00131\n","\n","Epoch 00059: val_loss improved from 0.00131 to 0.00130, saving model to SRCNN_check955.h5\n","\n","Epoch 00060: val_loss did not improve from 0.00130\n","\n","Epoch 00061: val_loss did not improve from 0.00130\n","\n","Epoch 00062: val_loss did not improve from 0.00130\n","\n","Epoch 00063: val_loss did not improve from 0.00130\n","\n","Epoch 00064: val_loss did not improve from 0.00130\n","\n","Epoch 00065: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check955.h5\n","\n","Epoch 00066: val_loss did not improve from 0.00130\n","\n","Epoch 00067: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check955.h5\n","\n","Epoch 00068: val_loss improved from 0.00130 to 0.00130, saving model to SRCNN_check955.h5\n","\n","Epoch 00069: val_loss did not improve from 0.00130\n","\n","Epoch 00070: val_loss improved from 0.00130 to 0.00129, saving model to SRCNN_check955.h5\n","\n","Epoch 00071: val_loss did not improve from 0.00129\n","\n","Epoch 00072: val_loss did not improve from 0.00129\n","\n","Epoch 00073: val_loss did not improve from 0.00129\n","\n","Epoch 00074: val_loss did not improve from 0.00129\n","\n","Epoch 00075: val_loss did not improve from 0.00129\n","\n","Epoch 00076: val_loss did not improve from 0.00129\n","\n","Epoch 00077: val_loss did not improve from 0.00129\n","\n","Epoch 00078: val_loss did not improve from 0.00129\n","\n","Epoch 00079: val_loss did not improve from 0.00129\n","\n","Epoch 00080: val_loss did not improve from 0.00129\n","\n","Epoch 00081: val_loss did not improve from 0.00129\n","\n","Epoch 00082: val_loss did not improve from 0.00129\n","\n","Epoch 00083: val_loss did not improve from 0.00129\n","\n","Epoch 00084: val_loss improved from 0.00129 to 0.00129, saving model to SRCNN_check955.h5\n","\n","Epoch 00085: val_loss did not improve from 0.00129\n","\n","Epoch 00086: val_loss improved from 0.00129 to 0.00128, saving model to SRCNN_check955.h5\n","\n","Epoch 00087: val_loss did not improve from 0.00128\n","\n","Epoch 00088: val_loss did not improve from 0.00128\n","\n","Epoch 00089: val_loss did not improve from 0.00128\n","\n","Epoch 00090: val_loss did not improve from 0.00128\n","\n","Epoch 00091: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check955.h5\n","\n","Epoch 00092: val_loss did not improve from 0.00128\n","\n","Epoch 00093: val_loss did not improve from 0.00128\n","\n","Epoch 00094: val_loss did not improve from 0.00128\n","\n","Epoch 00095: val_loss did not improve from 0.00128\n","\n","Epoch 00096: val_loss did not improve from 0.00128\n","\n","Epoch 00097: val_loss did not improve from 0.00128\n","\n","Epoch 00098: val_loss did not improve from 0.00128\n","\n","Epoch 00099: val_loss did not improve from 0.00128\n","\n","Epoch 00100: val_loss did not improve from 0.00128\n","\n","Epoch 00101: val_loss did not improve from 0.00128\n","\n","Epoch 00102: val_loss did not improve from 0.00128\n","\n","Epoch 00103: val_loss did not improve from 0.00128\n","\n","Epoch 00104: val_loss did not improve from 0.00128\n","\n","Epoch 00105: val_loss did not improve from 0.00128\n","\n","Epoch 00106: val_loss did not improve from 0.00128\n","\n","Epoch 00107: val_loss did not improve from 0.00128\n","\n","Epoch 00108: val_loss improved from 0.00128 to 0.00128, saving model to SRCNN_check955.h5\n","\n","Epoch 00109: val_loss did not improve from 0.00128\n","\n","Epoch 00110: val_loss did not improve from 0.00128\n","\n","Epoch 00111: val_loss did not improve from 0.00128\n","\n","Epoch 00112: val_loss did not improve from 0.00128\n","\n","Epoch 00113: val_loss improved from 0.00128 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00114: val_loss did not improve from 0.00127\n","\n","Epoch 00115: val_loss did not improve from 0.00127\n","\n","Epoch 00116: val_loss did not improve from 0.00127\n","\n","Epoch 00117: val_loss did not improve from 0.00127\n","\n","Epoch 00118: val_loss did not improve from 0.00127\n","\n","Epoch 00119: val_loss did not improve from 0.00127\n","\n","Epoch 00120: val_loss did not improve from 0.00127\n","\n","Epoch 00121: val_loss did not improve from 0.00127\n","\n","Epoch 00122: val_loss did not improve from 0.00127\n","\n","Epoch 00123: val_loss did not improve from 0.00127\n","\n","Epoch 00124: val_loss did not improve from 0.00127\n","\n","Epoch 00125: val_loss did not improve from 0.00127\n","\n","Epoch 00126: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00127: val_loss did not improve from 0.00127\n","\n","Epoch 00128: val_loss did not improve from 0.00127\n","\n","Epoch 00129: val_loss did not improve from 0.00127\n","\n","Epoch 00130: val_loss did not improve from 0.00127\n","\n","Epoch 00131: val_loss did not improve from 0.00127\n","\n","Epoch 00132: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00133: val_loss did not improve from 0.00127\n","\n","Epoch 00134: val_loss did not improve from 0.00127\n","\n","Epoch 00135: val_loss did not improve from 0.00127\n","\n","Epoch 00136: val_loss did not improve from 0.00127\n","\n","Epoch 00137: val_loss did not improve from 0.00127\n","\n","Epoch 00138: val_loss did not improve from 0.00127\n","\n","Epoch 00139: val_loss did not improve from 0.00127\n","\n","Epoch 00140: val_loss did not improve from 0.00127\n","\n","Epoch 00141: val_loss did not improve from 0.00127\n","\n","Epoch 00142: val_loss did not improve from 0.00127\n","\n","Epoch 00143: val_loss did not improve from 0.00127\n","\n","Epoch 00144: val_loss did not improve from 0.00127\n","\n","Epoch 00145: val_loss did not improve from 0.00127\n","\n","Epoch 00146: val_loss did not improve from 0.00127\n","\n","Epoch 00147: val_loss did not improve from 0.00127\n","\n","Epoch 00148: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00149: val_loss did not improve from 0.00127\n","\n","Epoch 00150: val_loss did not improve from 0.00127\n","\n","Epoch 00151: val_loss did not improve from 0.00127\n","\n","Epoch 00152: val_loss did not improve from 0.00127\n","\n","Epoch 00153: val_loss did not improve from 0.00127\n","\n","Epoch 00154: val_loss did not improve from 0.00127\n","\n","Epoch 00155: val_loss did not improve from 0.00127\n","\n","Epoch 00156: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00157: val_loss did not improve from 0.00127\n","\n","Epoch 00158: val_loss did not improve from 0.00127\n","\n","Epoch 00159: val_loss did not improve from 0.00127\n","\n","Epoch 00160: val_loss did not improve from 0.00127\n","\n","Epoch 00161: val_loss did not improve from 0.00127\n","\n","Epoch 00162: val_loss did not improve from 0.00127\n","\n","Epoch 00163: val_loss did not improve from 0.00127\n","\n","Epoch 00164: val_loss did not improve from 0.00127\n","\n","Epoch 00165: val_loss did not improve from 0.00127\n","\n","Epoch 00166: val_loss did not improve from 0.00127\n","\n","Epoch 00167: val_loss did not improve from 0.00127\n","\n","Epoch 00168: val_loss did not improve from 0.00127\n","\n","Epoch 00169: val_loss improved from 0.00127 to 0.00127, saving model to SRCNN_check955.h5\n","\n","Epoch 00170: val_loss did not improve from 0.00127\n","\n","Epoch 00171: val_loss did not improve from 0.00127\n","\n","Epoch 00172: val_loss did not improve from 0.00127\n","\n","Epoch 00173: val_loss did not improve from 0.00127\n","\n","Epoch 00174: val_loss improved from 0.00127 to 0.00126, saving model to SRCNN_check955.h5\n","\n","Epoch 00175: val_loss did not improve from 0.00126\n","\n","Epoch 00176: val_loss did not improve from 0.00126\n","\n","Epoch 00177: val_loss did not improve from 0.00126\n","\n","Epoch 00178: val_loss did not improve from 0.00126\n","\n","Epoch 00179: val_loss did not improve from 0.00126\n","\n","Epoch 00180: val_loss did not improve from 0.00126\n","\n","Epoch 00181: val_loss did not improve from 0.00126\n","\n","Epoch 00182: val_loss did not improve from 0.00126\n","\n","Epoch 00183: val_loss did not improve from 0.00126\n","\n","Epoch 00184: val_loss did not improve from 0.00126\n","\n","Epoch 00185: val_loss did not improve from 0.00126\n","\n","Epoch 00186: val_loss did not improve from 0.00126\n","\n","Epoch 00187: val_loss did not improve from 0.00126\n","\n","Epoch 00188: val_loss did not improve from 0.00126\n","\n","Epoch 00189: val_loss did not improve from 0.00126\n","\n","Epoch 00190: val_loss did not improve from 0.00126\n","\n","Epoch 00191: val_loss did not improve from 0.00126\n","\n","Epoch 00192: val_loss did not improve from 0.00126\n","\n","Epoch 00193: val_loss did not improve from 0.00126\n","\n","Epoch 00194: val_loss did not improve from 0.00126\n","\n","Epoch 00195: val_loss did not improve from 0.00126\n","\n","Epoch 00196: val_loss did not improve from 0.00126\n","\n","Epoch 00197: val_loss did not improve from 0.00126\n","\n","Epoch 00198: val_loss did not improve from 0.00126\n","\n","Epoch 00199: val_loss did not improve from 0.00126\n","\n","Epoch 00200: val_loss did not improve from 0.00126\n","[27.068928098388916, 29.313825226002567, 30.960749950597492, 30.9178422490896, 32.017932395572956, 32.26594916609456, 32.5771189425153, 32.69585024481455, 32.60954614446414, 32.75252437124192, 32.62235640445428, 32.90798029384953, 32.90798029384953, 32.956761883613886, 32.96923583308286, 32.96923583308286, 32.661447820006934, 33.01888723930886, 32.99901585031221, 33.02126531483498, 33.02126531483498, 33.02126531483498, 33.01165705797701, 33.089855040573056, 33.089855040573056, 32.775117209811434, 32.775117209811434, 32.775117209811434, 32.95438898643222, 33.115069370520665, 33.115069370520665, 33.115069370520665, 33.115069370520665, 32.758430339677346, 33.090294227686265, 33.090294227686265, 32.96048057026068, 32.96048057026068, 32.96048057026068, 33.08222610108411, 33.15054689187258, 33.15054689187258, 33.15054689187258, 33.15054689187258, 33.15054689187258, 33.15054689187258, 33.079034570307876, 33.079034570307876, 33.079034570307876, 33.079034570307876, 33.079034570307876, 33.25282997329037, 33.037960995970664, 33.037960995970664, 33.037960995970664, 33.037960995970664, 33.037960995970664, 33.037960995970664, 33.089369991069844, 33.089369991069844, 33.089369991069844, 33.089369991069844, 33.089369991069844, 33.089369991069844, 33.207786637919995, 33.207786637919995, 33.15903455206939, 33.08432778510754, 33.08432778510754, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.281191821757574, 33.007843283544794, 33.007843283544794, 33.10986368360525, 33.10986368360525, 33.10986368360525, 33.10986368360525, 33.10986368360525, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.15412552965425, 33.09505104264145, 33.09505104264145, 33.09505104264145, 33.09505104264145, 33.09505104264145, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.3464597136644, 33.24763996314097, 33.24763996314097, 33.24763996314097, 33.24763996314097, 33.24763996314097, 33.24763996314097, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29611687942107, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.29067230922654, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.34754534653369, 33.248218195698165, 33.248218195698165, 33.248218195698165, 33.248218195698165, 33.248218195698165, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349, 33.30146129824349]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"87FeW85u8L7c","colab_type":"code","outputId":"2d0d01e3-f649-488c-c238-a8fb95b2982d","executionInfo":{"status":"ok","timestamp":1580918312117,"user_tz":-330,"elapsed":1151,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":322}},"source":["print(history935.psnrs)\n","x1=numpy.linspace(1,len(history915.psnrs),len(history915.psnrs))\n","y1 = numpy.asarray(history915.psnrs, dtype=numpy.float32)\n","x2=numpy.linspace(1,len(history935.psnrs),len(history935.psnrs))\n","y2 = numpy.asarray(history935.psnrs, dtype=numpy.float32)\n","x3=numpy.linspace(1,len(history955.psnrs),len(history955.psnrs))\n","y3 = numpy.asarray(history955.psnrs, dtype=numpy.float32)\n","fig, ax = plt.subplots()\n","ax.plot(x1,y1, '-b', label='9-1-5')\n","ax.plot(x2,y2, '-r', label='9-3-5')\n","ax.plot(x3,y3,'-g',label='9-5-5')\n","#ax.axis('equal')\n","leg = ax.legend();\n","print(x.shape,y.shape)\n","plt.show(fig)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[25.09427695111339, 28.22092566500807, 29.76666444135584, 30.19559485877884, 31.56154906301195, 31.936467267927934, 31.71730220340454, 32.46787679556609, 32.45315982882129, 32.57004926560764, 32.721886091928525, 32.721886091928525, 33.06296682369763, 32.29678812655256, 32.815697161179905, 33.05824269958163, 33.08767447802629, 33.040680787030325, 32.99864837507955, 33.054315847251246, 33.054315847251246, 32.90165169574671, 32.90165169574671, 33.06345923319717, 33.02585227533511, 32.914509545248436, 33.09512743630711, 33.07679881873679, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.07579287330273, 33.09174910643072, 33.02219298386508, 33.138762550522635, 33.31045141744219, 33.31045141744219, 33.2649505770164, 33.2649505770164, 33.020135543376526, 33.020135543376526, 33.020135543376526, 33.04417826459989, 33.04417826459989, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.24934100558771, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.230205710537525, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 33.268744582651195, 32.97973453436559, 32.97973453436559, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.5152204196402, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.043888009317875, 33.10154092759482, 33.10154092759482, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.39740582986952, 33.07138066590796, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.350765953111235, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.386766663817305, 33.31905341213508, 33.31905341213508, 33.48243159846404, 33.48243159846404, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.280529751312876, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.25461044967322, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.06371801042626, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.35530414932194, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051, 33.21746304650051]\n","(200,) (200,)\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEuBCCGFREuBCCGFR/w8J0qSRshgzuQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"58XNJ9mMDjF8","colab_type":"code","outputId":"6b2dd165-2104-411b-b2e1-7e0c9d1319b4","executionInfo":{"status":"ok","timestamp":1580919127861,"user_tz":-330,"elapsed":18108,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":240}},"source":["predict(\"915\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(1, 1), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["(256, 256, 3) (256, 256)\n","(1, 244, 244, 1)\n","(256, 256, 3)\n","(244, 244, 3) (244, 244, 3)\n","bicubic:\n","YCrCCb= 32.053059313091204 , RGB=32.021477707251755\n","SRCNN:\n","YCrCCb= 33.230292091013126 , RGB=33.19740211220082\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"CXmcq3LSH5Oo","colab_type":"code","outputId":"d0a41c07-a186-4ae6-b0d8-1ad447749889","executionInfo":{"status":"ok","timestamp":1580919162912,"user_tz":-330,"elapsed":17917,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":240}},"source":["predict(\"935\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:46: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:48: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(3, 3), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:51: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["(256, 256, 3) (256, 256)\n","(1, 244, 244, 1)\n","(256, 256, 3)\n","(244, 244, 3) (244, 244, 3)\n","bicubic:\n","YCrCCb= 32.053059313091204 , RGB=32.021477707251755\n","SRCNN:\n","YCrCCb= 33.277545037629125 , RGB=33.23858124121794\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"wi8onmLIH5X0","colab_type":"code","outputId":"1cdf2e64-943b-4633-ed77-946b8a1e7961","executionInfo":{"status":"ok","timestamp":1580919203992,"user_tz":-330,"elapsed":17900,"user":{"displayName":"","photoUrl":"","userId":""}},"colab":{"base_uri":"https://localhost:8080/","height":240}},"source":["predict(\"955\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:74: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", input_shape=(None, Non..., kernel_size=(9, 9), filters=128, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"relu\", kernel_size=(5, 5), filters=64, padding=\"same\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n","/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:79: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(activation=\"linear\", kernel_size=(5, 5), filters=1, padding=\"valid\", kernel_initializer=\"glorot_uniform\", use_bias=True)`\n"],"name":"stderr"},{"output_type":"stream","text":["(256, 256, 3) (256, 256)\n","(1, 244, 244, 1)\n","(256, 256, 3)\n","(244, 244, 3) (244, 244, 3)\n","bicubic:\n","YCrCCb= 32.053059313091204 , RGB=32.021477707251755\n","SRCNN:\n","YCrCCb= 33.42842130964558 , RGB=33.408090681157795\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MXCPYnz3Kxx-","colab_type":"code","colab":{}},"source":["\n","def predict_model_rgb():\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(filters=128, kernel_size=(9,9), kernel_initializer='glorot_uniform',\n","                     activation='relu', padding='valid', use_bias=True, input_shape=(None, None, 3)))\n","    SRCNN.add(Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='glorot_uniform',\n","                     activation='relu', padding='same', use_bias=True))\n","    SRCNN.add(Conv2D(filters=3, kernel_size=(5,5), kernel_initializer='glorot_uniform',\n","                     activation='linear', padding='valid', use_bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","\n","\n","def model_rgb():\n","    SRCNN = Sequential()\n","    SRCNN.add(Conv2D(filters=128, kernel_size=(9,9), kernel_initializer='glorot_uniform',\n","                     activation='relu', padding='valid', use_bias=True, input_shape=(32, 32, 3)))\n","    SRCNN.add(Conv2D(filters=64, kernel_size=(3,3), kernel_initializer='glorot_uniform',\n","                     activation='relu', padding='same', use_bias=True))\n","    SRCNN.add(Conv2D(filters=3, kernel_size=(5,5), kernel_initializer='glorot_uniform',\n","                     activation='linear', padding='valid', use_bias=True))\n","    adam = Adam(lr=0.0003)\n","    SRCNN.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n","    return SRCNN\n","\n","class PsnrHistory(Callback):\n","    def on_train_begin(self, logs={}):\n","        self.psnrs = []\n","        \n","    def on_epoch_end(self, epoch, logs={}):\n","        rmse = math.sqrt(logs['val_loss'])\n","        psnr2=20 * math.log10(1/rmse)\n","        srcnn_model = predict_model_rgb()\n","        srcnn_model.load_weights(\"SRCNN_check_rgb.h5\")\n","        avg_psnr=0.0\n","        for i in range(0,data_rgb.shape[0]):\n","            img=data_rgb[i]\n","            Y = numpy.zeros((1, img.shape[0], img.shape[1], 3), dtype=float)\n","            Y[0, :, :, :] = img\n","            pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","            pre[pre[:] > 255] = 255\n","            pre[pre[:] < 0] = 0\n","            img_pre=pre[0,:,:,:]\n","            avg_psnr=avg_psnr+psnr(label_rgb[i]*255, img_pre)\n","        \n","        avg_psnr=(avg_psnr/data_rgb.shape[0])\n","        self.psnrs.append(avg_psnr)\n","\n","def train_rgb(history):\n","    srcnn_model = model_rgb()\n","    print(srcnn_model.summary())\n","    data, label = pd.read_training_data(\"./train_rgb.h5\")\n","    val_data, val_label = pd.read_training_data(\"./test_rgb.h5\")\n","\n","    checkpoint = ModelCheckpoint(\"SRCNN_check_rgb.h5\", monitor='val_loss', verbose=1, save_best_only=True,\n","                                 save_weights_only=False, mode='min')\n","    callbacks_list = [checkpoint,history]\n","\n","    srcnn_model.fit(data, label, batch_size=128, validation_data=(val_data, val_label),\n","                    callbacks=callbacks_list, shuffle=True, epochs=20, verbose=0)\n","    return history\n","    # srcnn_model.load_weights(\"m_model_adam.h5\")\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wgk_4B4Lm90L","colab_type":"code","outputId":"9377f52a-e905-4f9c-9c0a-38a36fceeb53","colab":{"base_uri":"https://localhost:8080/","height":989}},"source":["history=PsnrHistory()\n","train_rgb(history)\n","print(history.psnrs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Model: \"sequential_176\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","conv2d_526 (Conv2D)          (None, 24, 24, 128)       31232     \n","_________________________________________________________________\n","conv2d_527 (Conv2D)          (None, 24, 24, 64)        73792     \n","_________________________________________________________________\n","conv2d_528 (Conv2D)          (None, 20, 20, 3)         4803      \n","=================================================================\n","Total params: 109,827\n","Trainable params: 109,827\n","Non-trainable params: 0\n","_________________________________________________________________\n","None\n","\n","Epoch 00001: val_loss improved from inf to 0.00969, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00002: val_loss improved from 0.00969 to 0.00536, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00003: val_loss improved from 0.00536 to 0.00402, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00004: val_loss improved from 0.00402 to 0.00347, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00005: val_loss improved from 0.00347 to 0.00310, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00006: val_loss improved from 0.00310 to 0.00288, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00007: val_loss improved from 0.00288 to 0.00275, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00008: val_loss improved from 0.00275 to 0.00270, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00009: val_loss improved from 0.00270 to 0.00249, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00010: val_loss improved from 0.00249 to 0.00241, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00011: val_loss improved from 0.00241 to 0.00232, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00012: val_loss improved from 0.00232 to 0.00228, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00013: val_loss improved from 0.00228 to 0.00220, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00014: val_loss improved from 0.00220 to 0.00217, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00015: val_loss improved from 0.00217 to 0.00211, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00016: val_loss improved from 0.00211 to 0.00210, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00017: val_loss did not improve from 0.00210\n","\n","Epoch 00018: val_loss improved from 0.00210 to 0.00204, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00019: val_loss improved from 0.00204 to 0.00204, saving model to SRCNN_check_rgb.h5\n","\n","Epoch 00020: val_loss improved from 0.00204 to 0.00203, saving model to SRCNN_check_rgb.h5\n","[22.109002782387293, 25.948106603722053, 27.438752078114597, 28.005991542947932, 28.764151133702015, 29.168372201536947, 29.31588765003348, 29.255593170178784, 29.995164950885982, 30.08758984916471, 30.186229182372852, 30.19890304840734, 30.467734257610353, 30.430116796391967, 30.56293988851693, 30.697628418343818, 30.697628418343818, 30.987388482581977, 30.76002956408116, 30.64541008370373]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ChaRMznRAtgt","colab_type":"code","colab":{}},"source":["def predict_rgb():\n","    srcnn_model = predict_model_rgb()\n","    srcnn_model.load_weights(\"SRCNN_check_rgb.h5\")\n","    IMG_NAME = \"flower.png\"\n","    INPUT_NAME = \"input2_rgb.png\"\n","    OUTPUT_NAME = \"pre_rgb.png\"\n","\n","    import cv2\n","    img = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)\n","    shape = img.shape\n","    Y_img = cv2.resize(img[:, :, :], (int(shape[1] / 2), int(shape[0] / 2)), cv2.INTER_CUBIC)\n","    Y_img = cv2.resize(Y_img, (shape[1], shape[0]), cv2.INTER_CUBIC)\n","    img=Y_img\n","    print(Y_img.shape)\n","    cv2.imwrite(INPUT_NAME, Y_img)\n","\n","    Y = numpy.zeros((1, img.shape[0], img.shape[1], 3), dtype=float)\n","    Y[0, :, :, :] = Y_img.astype(float) / 255.\n","    pre = srcnn_model.predict(Y, batch_size=1) * 255.\n","    pre[pre[:] > 255] = 255\n","    pre[pre[:] < 0] = 0\n","    pre = pre.astype(numpy.uint8)\n","    img[6: -6, 6: -6,:]=pre[0,:,:,:]\n","    print(img.shape)\n","    cv2.imwrite(OUTPUT_NAME, img)\n","\n","\n","    # psnr calculation:\n","    im1_rgb = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6]\n","    im1_Y = cv2.cvtColor(im1_rgb, cv2.COLOR_BGR2YCrCb)\n","    im2_rgb = cv2.imread(INPUT_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6]\n","    im2_Y = cv2.cvtColor(im2_rgb, cv2.COLOR_BGR2YCrCb)\n","    im3_rgb = cv2.imread(OUTPUT_NAME, cv2.IMREAD_COLOR)[6: -6, 6: -6]\n","    im3_Y = cv2.cvtColor(im3_rgb, cv2.COLOR_BGR2YCrCb)\n","\n","    print(im1_Y.shape,im2_Y.shape)\n","    print(\"bicubic:\")\n","    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im2_Y[:,:,0]),psnr(im1_rgb,im2_rgb)))\n","    print(\"SRCNN:\")\n","    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im3_Y[:,:,0]),psnr(im1_rgb,im3_rgb)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hKLiD2UAHALF","colab_type":"code","outputId":"aa5e40d8-de87-411b-fdbd-68b2ca0121bc","colab":{"base_uri":"https://localhost:8080/","height":299}},"source":["x=numpy.linspace(1,len(history.psnrs),len(history.psnrs))\n","y = numpy.asarray(history.psnrs, dtype=numpy.float32)\n","print(x[0],y.shape)\n","plt.plot(x,history.psnrs)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["1.0 (20,)\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["[<matplotlib.lines.Line2D at 0x7fb145bb83c8>]"]},"metadata":{"tags":[]},"execution_count":194},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"nc6-ayWYUpAI","colab_type":"code","outputId":"c773476f-9d83-4d50-bc73-a8c7cc8af076","colab":{"base_uri":"https://localhost:8080/","height":136}},"source":["predict_rgb()\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["(276, 276, 3)\n","(276, 276, 3)\n","(264, 264, 3) (264, 264, 3)\n","bicubic:\n","YCrCCb= 33.577765973062625 , RGB=32.56036165104796\n","SRCNN:\n","YCrCCb= 33.87933062993204 , RGB=32.57475283303932\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"q5z7uMHHUrtN","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}