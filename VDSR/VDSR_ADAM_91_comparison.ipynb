{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "VDSR_ADAM_91.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjix9gtJ11_7",
        "colab_type": "code",
        "outputId": "1235eda1-2aef-422a-9da7-b277567c98f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "crop_train_291.h5\t  pre15.png\t   Train\n",
            "crop_train.h5\t\t  pre20.png\t   VDSR_ADAM_291.h5\n",
            "crop_train_no_overlap.h5  pre2.png\t   VDSR_ADAM_91_10.h5\n",
            "input10.png\t\t  pre5.png\t   VDSR_ADAM_91_15.h5\n",
            "input15.png\t\t  prepare_data.py  VDSR_ADAM_91_20.h5\n",
            "input20.png\t\t  __pycache__\t   VDSR_ADAM_91_5.h5\n",
            "input2.png\t\t  t1.png\t   VDSR_check.h5\n",
            "input5.png\t\t  Test\n",
            "pre10.png\t\t  test.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUZYPGVa2CzZ",
        "colab_type": "code",
        "outputId": "c1ba93fa-0861-4cd8-b118-eeb8822da46c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import os \n",
        "os.chdir(\"/content/drive/My Drive/Image_super_resolution/VDSR_implementation\")\n",
        "os.getcwd()\n",
        "!ls"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "crop_train_291.h5\t  pre15.png\t   Train\n",
            "crop_train.h5\t\t  pre20.png\t   VDSR_ADAM_291.h5\n",
            "crop_train_no_overlap.h5  pre2.png\t   VDSR_ADAM_91_10.h5\n",
            "input10.png\t\t  pre5.png\t   VDSR_ADAM_91_15.h5\n",
            "input15.png\t\t  prepare_data.py  VDSR_ADAM_91_20.h5\n",
            "input20.png\t\t  __pycache__\t   VDSR_ADAM_91_5.h5\n",
            "input2.png\t\t  t1.png\t   VDSR_check.h5\n",
            "input5.png\t\t  Test\n",
            "pre10.png\t\t  test.h5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1tk0ftM2cu7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Conv2D, Input, BatchNormalization, Activation,add\n",
        "# from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import ModelCheckpoint,Callback, LearningRateScheduler\n",
        "from keras.optimizers import SGD, Adam\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import prepare_data as pd\n",
        "import numpy \n",
        "import math\n",
        "from keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsQdWZ_E2kpW",
        "colab_type": "code",
        "outputId": "944c1098-2227-4f0c-894b-3379106519fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "IMG_SIZE = (41, 41, 1)\n",
        "SIZE=10000\n",
        "%matplotlib inline\n",
        "data,label=pd.read_training_data(\"crop_train.h5\")\n",
        "print(data.shape,label.shape)\n",
        "# print(data[0][0][0][0])\n",
        "# data_img=data[5]\n",
        "# label_img=label[5]\n",
        "# data=data\n",
        "# label=label\n",
        "# fig,axes=plt.subplots(1,2)\n",
        "# axes[0].imshow(data_img)\n",
        "# axes[1].imshow(label_img)\n",
        "data_Y, label_Y = pd.read_training_data(\"./test.h5\")\n",
        "print(data_Y.shape,label_Y.shape)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(13717, 41, 41, 1) (13717, 41, 41, 1)\n",
            "(420, 41, 41, 1) (420, 41, 41, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQH34baJeBvw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def psnr(target, ref):\n",
        "    mse = numpy.mean( (target - ref) ** 2 )\n",
        "    PIXEL_MAX = 255.0\n",
        "    if mse == 0:\n",
        "        return 100\n",
        "    return 20 * math.log10(PIXEL_MAX / math.sqrt(mse))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SF5DOvSPey12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(num_layers):\n",
        "    \n",
        "    input_img = Input(shape=IMG_SIZE)\n",
        "    VDSR = Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_uniform')(input_img)\n",
        "    for i in range(num_layers-2):\n",
        "        VDSR = Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_uniform')(VDSR)\n",
        "        VDSR = Activation('relu')(VDSR)\n",
        "    VDSR = Conv2D(1, (3, 3), padding='same', kernel_initializer='glorot_uniform')(VDSR)\n",
        "    res_img = VDSR\n",
        "\n",
        "    output_img = add([res_img, input_img])\n",
        "    VDSR = Model(input_img, output_img)\n",
        "    adam = Adam(clipvalue=0.5)\n",
        "    VDSR.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return VDSR\n",
        "\n",
        "def predict_model(num_layers):\n",
        "    \n",
        "    input_img = Input(shape=(None,None,1))\n",
        "    VDSR = Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_uniform')(input_img)\n",
        "    for i in range(num_layers-2):\n",
        "        VDSR = Conv2D(64, (3, 3), padding='same', kernel_initializer='glorot_uniform')(VDSR)\n",
        "        VDSR = Activation('relu')(VDSR)\n",
        "    VDSR = Conv2D(1, (3, 3), padding='same', kernel_initializer='glorot_uniform')(VDSR)\n",
        "    res_img = VDSR\n",
        "\n",
        "    output_img = add([res_img, input_img])\n",
        "    VDSR = Model(input_img, output_img)\n",
        "    adam = Adam(clipvalue=0.5)\n",
        "    VDSR.compile(optimizer=adam, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "    return VDSR\n",
        "\n",
        "\n",
        "def step_decay(epoch):\n",
        "   initial_lrate = 0.0003\n",
        "   drop = 1\n",
        "   epochs_drop = 20.0\n",
        "   lrate = initial_lrate * math.pow(drop,math.floor(epoch/epochs_drop))\n",
        "   return lrate\n",
        "\n",
        "class LossHistory(Callback):\n",
        "    def on_train_begin(self, logs={}):\n",
        "       self.t_losses = []\n",
        "       self.v_losses=[]\n",
        "       self.lr = []\n",
        " \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "       self.v_losses.append(logs.get('val_loss'))\n",
        "       self.t_losses.append(logs.get('loss'))\n",
        "       self.lr.append(step_decay(epoch))\n",
        "    \n",
        "class PsnrHistory_Y(Callback):\n",
        "    def __init__(self,num_layers):\n",
        "        self.num_layers=num_layers\n",
        "\n",
        "    def on_train_begin(self, logs={}):\n",
        "        self.psnrs = []\n",
        "        \n",
        "    def on_epoch_end(self, epoch, logs={}):\n",
        "        VDSR = predict_model(self.num_layers)\n",
        "        VDSR.load_weights(\"VDSR_ADAM_91\"+\"_\"+str(self.num_layers)+\".h5\")\n",
        "        avg_psnr=0.0\n",
        "        import cv2\n",
        "        for i in range(0,data_Y.shape[0]):\n",
        "            img=data_Y[i]\n",
        "            Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "            Y[0, :, :, :] = img\n",
        "            pre = VDSR.predict(Y, batch_size=1) * 255.\n",
        "            pre[pre[:] > 255] = 255\n",
        "            pre[pre[:] < 0] = 0\n",
        "            img_pre=pre[0,:,:,:]\n",
        "            avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n",
        "\n",
        "        avg_psnr=(avg_psnr/data_Y.shape[0])        \n",
        "        self.psnrs.append((avg_psnr))\n",
        "\n",
        "def train(num_layers,history):\n",
        "    VDSR = model(num_layers)\n",
        "    print(VDSR.summary())\n",
        "    data, label = pd.read_training_data(\"./crop_train.h5\")\n",
        "    val_data, val_label = pd.read_training_data(\"./test.h5\")\n",
        "    # data =data[0:SIZE]\n",
        "    # label=label[0:SIZE]\n",
        "    print(data.shape,label.shape)\n",
        "    print(val_data.shape,val_label.shape)\n",
        "    loss_history = LossHistory()\n",
        "    lrate = LearningRateScheduler(step_decay,verbose=1)\n",
        "    checkpoint = ModelCheckpoint(\"VDSR_ADAM_91\"+\"_\"+str(num_layers)+\".h5\", monitor='val_loss', verbose=1, save_best_only=True,\n",
        "                                 save_weights_only=False, mode='min')\n",
        "    callbacks_list = [checkpoint,loss_history, lrate,history]\n",
        "    VDSR.fit(data, label, batch_size=64, validation_data=(val_data, val_label),\n",
        "                    callbacks=callbacks_list, shuffle=True, nb_epoch=40, verbose=0)\n",
        "    print(loss_history.lr)\n",
        "    print(loss_history.t_losses)\n",
        "    print(loss_history.v_losses)\n",
        "    # srcnn_model.load_weights(\"m_model_adam.h5\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uQhC8yXWEuKj",
        "colab_type": "code",
        "outputId": "ba835f5d-607e-4e2e-d223-94a5cbd7eab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history_Y_5=PsnrHistory_Y(5)\n",
        "train(5,history_Y_5)\n",
        "print(history_Y_5.psnrs)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_25\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_25 (InputLayer)           (None, 41, 41, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_162 (Conv2D)             (None, 41, 41, 64)   640         input_25[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_163 (Conv2D)             (None, 41, 41, 64)   36928       conv2d_162[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_127 (Activation)     (None, 41, 41, 64)   0           conv2d_163[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_164 (Conv2D)             (None, 41, 41, 64)   36928       activation_127[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_128 (Activation)     (None, 41, 41, 64)   0           conv2d_164[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_165 (Conv2D)             (None, 41, 41, 64)   36928       activation_128[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_129 (Activation)     (None, 41, 41, 64)   0           conv2d_165[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_166 (Conv2D)             (None, 41, 41, 1)    577         activation_129[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_25 (Add)                    (None, 41, 41, 1)    0           conv2d_166[0][0]                 \n",
            "                                                                 input_25[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 112,001\n",
            "Trainable params: 112,001\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "(13717, 41, 41, 1) (13717, 41, 41, 1)\n",
            "(420, 41, 41, 1) (420, 41, 41, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00154, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00154 to 0.00147, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00147 to 0.00144, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00144 to 0.00141, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00141 to 0.00140, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00140 to 0.00139, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00139 to 0.00138, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00138 to 0.00136, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.00136 to 0.00136, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00136 to 0.00136, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.00136 to 0.00135, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.00135 to 0.00135, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.00135 to 0.00135, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.00135 to 0.00134, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00134 to 0.00134, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.00134 to 0.00134, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.00134 to 0.00134, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00134\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.00134 to 0.00133, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.00133 to 0.00133, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.00133 to 0.00133, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.00133 to 0.00133, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.00133 to 0.00133, saving model to VDSR_ADAM_91_5.h5\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00133\n",
            "[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]\n",
            "[0.0013637548219137588, 0.0011880101359191255, 0.0011412043209069072, 0.0011080999445889721, 0.0010843571592281532, 0.0010663413566342952, 0.0010529711258618652, 0.0010402773661927864, 0.0010291299021126536, 0.0010210254615659012, 0.0010121275819755214, 0.0010045732713210153, 0.000998162189493112, 0.0009912291585810697, 0.0009862318484819006, 0.000980622887032682, 0.0009751015225075158, 0.0009706781921820538, 0.0009663319919594304, 0.0009620567103222204, 0.0009581323717282201, 0.0009549764754734944, 0.0009510495241545643, 0.0009470269720038764, 0.0009439535532761147, 0.0009411557042847887, 0.000937650139232625, 0.0009343791273371957, 0.0009317273960054736, 0.00092795887424455, 0.0009256456151057822, 0.0009234790016711396, 0.0009206358929326435, 0.0009178401043024065, 0.0009163235415320736, 0.0009133176723131681, 0.0009108917231046806, 0.0009087548678264029, 0.0009057558520641451, 0.000903854301902239]\n",
            "[0.001539513811336032, 0.0014734135985019661, 0.0014364880975335837, 0.0014125749025316466, 0.0013967095719029507, 0.0013857830593007661, 0.001376379360001357, 0.0013648573131788346, 0.0013602637885404484, 0.0013568816961543192, 0.001354247016743535, 0.001352545368440804, 0.0013458566917549995, 0.0013447841185899008, 0.0013439674950426533, 0.0013417375905971443, 0.0013373304923464146, 0.0013439291289874485, 0.0013343615546112968, 0.001336850597345758, 0.0013346750799211717, 0.0013359135648767864, 0.0013325317629746028, 0.0013354875590829622, 0.0013485399584862447, 0.0013344632317533805, 0.0013312523674574635, 0.0013419663600091423, 0.0013288341540222367, 0.001330142724327743, 0.0013304111581029638, 0.0013339762531575702, 0.0013297969686044823, 0.0013337505659798072, 0.001331313903487864, 0.0013283317498419256, 0.001335706524107428, 0.0013310895955544852, 0.0013378863012240756, 0.00133617764915384]\n",
            "[31.47185295216429, 31.686849374947286, 31.75902240648939, 31.922175251405584, 31.9598942265459, 32.03301590855234, 32.03912156609697, 32.12454821998926, 32.134263908471645, 32.139489255945, 32.17689052207238, 32.14807564426086, 32.231530955490946, 32.21405844199198, 32.237977770118086, 32.2828668602716, 32.2569365874283, 32.2569365874283, 32.27629702179068, 32.27629702179068, 32.27629702179068, 32.27629702179068, 32.34459935817683, 32.34459935817683, 32.34459935817683, 32.34459935817683, 32.30105030229579, 32.30105030229579, 32.341666871897274, 32.341666871897274, 32.341666871897274, 32.341666871897274, 32.341666871897274, 32.341666871897274, 32.341666871897274, 32.396533718843976, 32.396533718843976, 32.396533718843976, 32.396533718843976, 32.396533718843976]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j7iQ2y5Jw4A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f02dbfd-8a6a-4c1d-8c4d-54dec5fb9108"
      },
      "source": [
        "history_Y_10=PsnrHistory_Y(10)\n",
        "train(10,history_Y_10)\n",
        "print(history_Y_10.psnrs)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_66\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_66 (InputLayer)           (None, 41, 41, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_367 (Conv2D)             (None, 41, 41, 64)   640         input_66[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_368 (Conv2D)             (None, 41, 41, 64)   36928       conv2d_367[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_250 (Activation)     (None, 41, 41, 64)   0           conv2d_368[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_369 (Conv2D)             (None, 41, 41, 64)   36928       activation_250[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_251 (Activation)     (None, 41, 41, 64)   0           conv2d_369[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_370 (Conv2D)             (None, 41, 41, 64)   36928       activation_251[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_252 (Activation)     (None, 41, 41, 64)   0           conv2d_370[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_371 (Conv2D)             (None, 41, 41, 64)   36928       activation_252[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_253 (Activation)     (None, 41, 41, 64)   0           conv2d_371[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_372 (Conv2D)             (None, 41, 41, 64)   36928       activation_253[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_254 (Activation)     (None, 41, 41, 64)   0           conv2d_372[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_373 (Conv2D)             (None, 41, 41, 64)   36928       activation_254[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_255 (Activation)     (None, 41, 41, 64)   0           conv2d_373[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_374 (Conv2D)             (None, 41, 41, 64)   36928       activation_255[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_256 (Activation)     (None, 41, 41, 64)   0           conv2d_374[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_375 (Conv2D)             (None, 41, 41, 64)   36928       activation_256[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_257 (Activation)     (None, 41, 41, 64)   0           conv2d_375[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_376 (Conv2D)             (None, 41, 41, 1)    577         activation_257[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_66 (Add)                    (None, 41, 41, 1)    0           conv2d_376[0][0]                 \n",
            "                                                                 input_66[0][0]                   \n",
            "==================================================================================================\n",
            "Total params: 296,641\n",
            "Trainable params: 296,641\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "(13717, 41, 41, 1) (13717, 41, 41, 1)\n",
            "(420, 41, 41, 1) (420, 41, 41, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00151, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00151 to 0.00142, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00142 to 0.00137, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00137 to 0.00135, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.00135\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00135 to 0.00134, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00134 to 0.00133, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00133 to 0.00132, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00132 to 0.00132, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.00132 to 0.00131, saving model to VDSR_ADAM_91_10.h5\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00131\n",
            "[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]\n",
            "[0.0013413249871346538, 0.0011311872800443589, 0.0010504056961183842, 0.0010118247770752602, 0.0009861416653758153, 0.000965144462384135, 0.0009499168784287064, 0.000935883006065403, 0.00092200526859589, 0.0009112677949641557, 0.0009014758130613835, 0.0008928612539795032, 0.0008818738646587002, 0.0008755881810710057, 0.000866562687568372, 0.0008583357080646058, 0.000848463968982654, 0.0008424694052250485, 0.0008348055594943813, 0.0008314963012065755, 0.0008173031828927987, 0.0008103872918730047, 0.0008015267883819944, 0.0007957858169608738, 0.0007900294191453358, 0.0007788360724341062, 0.0007729900583147263, 0.0007658367623553339, 0.000758631192356171, 0.000752685400363538, 0.0007430247138622338, 0.0007353026857065618, 0.0007270993078625517, 0.0007223118957343732, 0.0007219878585108918, 0.0007082363697278911, 0.000707220529727229, 0.0006991520679819675, 0.0006884554319510312, 0.0006823795422738646]\n",
            "[0.0015103163585687676, 0.0014164326967494119, 0.0013678215577134065, 0.001350507317554383, 0.0013539882842451335, 0.001341640381585984, 0.0013304702571726272, 0.0013234717172703574, 0.00132421276177324, 0.001315884261081616, 0.0013218434616213753, 0.0013257930398963037, 0.0013245622527652554, 0.0013238085478189446, 0.0013106438885664656, 0.0013361797279988726, 0.001312360806124551, 0.001326925834153025, 0.0013206705783626862, 0.0013244499020012362, 0.0013346065013181596, 0.0013539029462706475, 0.0013395782593371612, 0.0013435947897267484, 0.0013429012593059312, 0.0013730230186844157, 0.0013640253982018856, 0.0013755421265072767, 0.0013904184391278595, 0.0013573847821958009, 0.0013949728049781351, 0.0013802399659263236, 0.001400122514349364, 0.0013934399866099869, 0.0013567697079408737, 0.001414651340538902, 0.0013985499407031705, 0.001448544860994887, 0.001438177962388311, 0.0014401227002963425]\n",
            "[31.57186346709363, 31.919032699045882, 32.133824761307324, 32.20997055335011, 32.20997055335011, 32.25972036610027, 32.336931122809375, 32.38792462157342, 32.38792462157342, 32.4330818708133, 32.4330818708133, 32.4330818708133, 32.4330818708133, 32.4330818708133, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956, 32.45105989834956]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M1cEtDgeJXfK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "ebb1a3ab-2752-4bdd-eb4e-42ea4094553b"
      },
      "source": [
        "history_Y_20=PsnrHistory_Y(20)\n",
        "train(20,history_Y_20)\n",
        "print(history_Y_20.psnrs)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_107\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_107 (InputLayer)          (None, 41, 41, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_777 (Conv2D)             (None, 41, 41, 64)   640         input_107[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_778 (Conv2D)             (None, 41, 41, 64)   36928       conv2d_777[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "activation_578 (Activation)     (None, 41, 41, 64)   0           conv2d_778[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_779 (Conv2D)             (None, 41, 41, 64)   36928       activation_578[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_579 (Activation)     (None, 41, 41, 64)   0           conv2d_779[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_780 (Conv2D)             (None, 41, 41, 64)   36928       activation_579[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_580 (Activation)     (None, 41, 41, 64)   0           conv2d_780[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_781 (Conv2D)             (None, 41, 41, 64)   36928       activation_580[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_581 (Activation)     (None, 41, 41, 64)   0           conv2d_781[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_782 (Conv2D)             (None, 41, 41, 64)   36928       activation_581[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_582 (Activation)     (None, 41, 41, 64)   0           conv2d_782[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_783 (Conv2D)             (None, 41, 41, 64)   36928       activation_582[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_583 (Activation)     (None, 41, 41, 64)   0           conv2d_783[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_784 (Conv2D)             (None, 41, 41, 64)   36928       activation_583[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_584 (Activation)     (None, 41, 41, 64)   0           conv2d_784[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_785 (Conv2D)             (None, 41, 41, 64)   36928       activation_584[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_585 (Activation)     (None, 41, 41, 64)   0           conv2d_785[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_786 (Conv2D)             (None, 41, 41, 64)   36928       activation_585[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_586 (Activation)     (None, 41, 41, 64)   0           conv2d_786[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_787 (Conv2D)             (None, 41, 41, 64)   36928       activation_586[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_587 (Activation)     (None, 41, 41, 64)   0           conv2d_787[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_788 (Conv2D)             (None, 41, 41, 64)   36928       activation_587[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_588 (Activation)     (None, 41, 41, 64)   0           conv2d_788[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_789 (Conv2D)             (None, 41, 41, 64)   36928       activation_588[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_589 (Activation)     (None, 41, 41, 64)   0           conv2d_789[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_790 (Conv2D)             (None, 41, 41, 64)   36928       activation_589[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_590 (Activation)     (None, 41, 41, 64)   0           conv2d_790[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_791 (Conv2D)             (None, 41, 41, 64)   36928       activation_590[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_591 (Activation)     (None, 41, 41, 64)   0           conv2d_791[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_792 (Conv2D)             (None, 41, 41, 64)   36928       activation_591[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_592 (Activation)     (None, 41, 41, 64)   0           conv2d_792[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_793 (Conv2D)             (None, 41, 41, 64)   36928       activation_592[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_593 (Activation)     (None, 41, 41, 64)   0           conv2d_793[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_794 (Conv2D)             (None, 41, 41, 64)   36928       activation_593[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_594 (Activation)     (None, 41, 41, 64)   0           conv2d_794[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_795 (Conv2D)             (None, 41, 41, 64)   36928       activation_594[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "activation_595 (Activation)     (None, 41, 41, 64)   0           conv2d_795[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_796 (Conv2D)             (None, 41, 41, 1)    577         activation_595[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "add_107 (Add)                   (None, 41, 41, 1)    0           conv2d_796[0][0]                 \n",
            "                                                                 input_107[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 665,921\n",
            "Trainable params: 665,921\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "(13717, 41, 41, 1) (13717, 41, 41, 1)\n",
            "(420, 41, 41, 1) (420, 41, 41, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00149, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00149 to 0.00143, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00143 to 0.00140, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.00140 to 0.00137, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00137 to 0.00134, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00134 to 0.00133, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.00133 to 0.00133, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00133\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.00133 to 0.00132, saving model to VDSR_ADAM_91_20.h5\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00132\n",
            "[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]\n",
            "[0.0013281575088283165, 0.0011072966798019294, 0.001029219922553851, 0.0009912025868029583, 0.0009600078676284902, 0.0009378587542672199, 0.0009216890837755491, 0.0009036763568181405, 0.0008876498123108968, 0.0008700630561790284, 0.0008529150322053654, 0.000840985614174069, 0.000824170244924444, 0.0008071528123141045, 0.0007952940358757986, 0.0007718684758508334, 0.0007592697357898405, 0.0007411452676447606, 0.0007350901550451209, 0.0007106578167603075, 0.0006968938563589835, 0.000683163499651497, 0.0006681914581610543, 0.0006535214970622348, 0.0006371156781920812, 0.0006295157855329006, 0.0006168546055449968, 0.000605978370709875, 0.0005932478862672025, 0.0005838146522330085, 0.000575943793699705, 0.0005758316255596684, 0.0005572907149416867, 0.0005558877625645769, 0.0005430966869533912, 0.0005297098327782687, 0.0005270443350886193, 0.0005180443191941654, 0.0005109044331540553, 0.0005018428367263479]\n",
            "[0.001487149874724093, 0.0014260677620768548, 0.0014021892854500385, 0.001370962682579245, 0.0013445001749676608, 0.0013312523674574635, 0.0013262520289225948, 0.0013470198260620236, 0.0013435010277178315, 0.0013179189747288113, 0.0013524137492779466, 0.0013397431932389737, 0.0013580564731022432, 0.0013475877166326557, 0.0014107014462795286, 0.0014099837691035298, 0.0013499496271833777, 0.0014699788619985893, 0.0014033993695019965, 0.0013808004114599455, 0.001406874267073969, 0.0014057694934308529, 0.0014177418513489622, 0.00143167432397604, 0.0014190986985340714, 0.0015051219157785887, 0.001419157775429388, 0.0014528991787561348, 0.0015048504540962832, 0.0014751750022350323, 0.0015690672787882032, 0.0014521398603738773, 0.0015054120637831233, 0.001525577013602569, 0.0015272819026861163, 0.0014702532429336792, 0.001582001350332229, 0.0014884778886057792, 0.0015544515236147812, 0.0015032733823837979]\n",
            "[31.602204511888488, 31.875219911297943, 31.964457375480787, 32.14449600247175, 32.29208865251393, 32.36679810411994, 32.4085922408039, 32.4085922408039, 32.4085922408039, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544, 32.4400672770544]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9w-vRd7sJ-08",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "32af0928-5017-4622-fc20-265d987c7ba6"
      },
      "source": [
        "history_Y_15=PsnrHistory_Y(15)\n",
        "train(15,history_Y_15)\n",
        "print(history_Y_15.psnrs)"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_148\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_148 (InputLayer)          (None, 41, 41, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1597 (Conv2D)            (None, 41, 41, 64)   640         input_148[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1598 (Conv2D)            (None, 41, 41, 64)   36928       conv2d_1597[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "activation_1316 (Activation)    (None, 41, 41, 64)   0           conv2d_1598[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1599 (Conv2D)            (None, 41, 41, 64)   36928       activation_1316[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1317 (Activation)    (None, 41, 41, 64)   0           conv2d_1599[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1600 (Conv2D)            (None, 41, 41, 64)   36928       activation_1317[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1318 (Activation)    (None, 41, 41, 64)   0           conv2d_1600[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1601 (Conv2D)            (None, 41, 41, 64)   36928       activation_1318[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1319 (Activation)    (None, 41, 41, 64)   0           conv2d_1601[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1602 (Conv2D)            (None, 41, 41, 64)   36928       activation_1319[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1320 (Activation)    (None, 41, 41, 64)   0           conv2d_1602[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1603 (Conv2D)            (None, 41, 41, 64)   36928       activation_1320[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1321 (Activation)    (None, 41, 41, 64)   0           conv2d_1603[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1604 (Conv2D)            (None, 41, 41, 64)   36928       activation_1321[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1322 (Activation)    (None, 41, 41, 64)   0           conv2d_1604[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1605 (Conv2D)            (None, 41, 41, 64)   36928       activation_1322[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1323 (Activation)    (None, 41, 41, 64)   0           conv2d_1605[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1606 (Conv2D)            (None, 41, 41, 64)   36928       activation_1323[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1324 (Activation)    (None, 41, 41, 64)   0           conv2d_1606[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1607 (Conv2D)            (None, 41, 41, 64)   36928       activation_1324[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1325 (Activation)    (None, 41, 41, 64)   0           conv2d_1607[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1608 (Conv2D)            (None, 41, 41, 64)   36928       activation_1325[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1326 (Activation)    (None, 41, 41, 64)   0           conv2d_1608[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1609 (Conv2D)            (None, 41, 41, 64)   36928       activation_1326[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1327 (Activation)    (None, 41, 41, 64)   0           conv2d_1609[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1610 (Conv2D)            (None, 41, 41, 64)   36928       activation_1327[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "activation_1328 (Activation)    (None, 41, 41, 64)   0           conv2d_1610[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1611 (Conv2D)            (None, 41, 41, 1)    577         activation_1328[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "add_148 (Add)                   (None, 41, 41, 1)    0           conv2d_1611[0][0]                \n",
            "                                                                 input_148[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 481,281\n",
            "Trainable params: 481,281\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "None\n",
            "(13717, 41, 41, 1) (13717, 41, 41, 1)\n",
            "(420, 41, 41, 1) (420, 41, 41, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:92: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 00001: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.00146, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00002: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.00146 to 0.00140, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00003: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.00140 to 0.00136, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00004: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.00136\n",
            "\n",
            "Epoch 00005: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.00136 to 0.00133, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00006: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.00133 to 0.00132, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00007: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.00132\n",
            "\n",
            "Epoch 00008: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.00132 to 0.00131, saving model to VDSR_ADAM_91_15.h5\n",
            "\n",
            "Epoch 00009: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00010: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00011: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00012: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00013: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00014: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00015: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00016: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00017: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00018: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00019: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00020: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00021: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00022: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00023: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00024: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00025: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00026: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00027: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00028: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00029: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00030: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00031: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00032: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00033: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00034: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00035: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00036: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00037: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00038: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00039: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.00131\n",
            "\n",
            "Epoch 00040: LearningRateScheduler setting learning rate to 0.0003.\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.00131\n",
            "[0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003]\n",
            "[0.0012887800484434016, 0.0010828377856689862, 0.001016013671703203, 0.0009753942976341403, 0.0009507103356343799, 0.0009294678423727653, 0.0009115568401878312, 0.0008927673283758537, 0.0008783558161786398, 0.0008644123522453692, 0.0008492428273191175, 0.0008362507110465259, 0.000823146375461908, 0.0008056497681870885, 0.000787785073415082, 0.0007785930193081692, 0.0007617497998548446, 0.0007453980783241685, 0.000730296050404456, 0.0007194919704490568, 0.0007050434408825915, 0.0006962518885202893, 0.0006807659749068489, 0.000669953443364156, 0.0006587561898631041, 0.0006468726336380814, 0.0006384728259351717, 0.000625601281754852, 0.0006192836945129901, 0.0006128591205719365, 0.0006023945282100344, 0.000594612054057037, 0.000592455335810176, 0.0005818067955108396, 0.0005744131426173487, 0.0005662890660384786, 0.0005603480446170721, 0.0005571448386039074, 0.000551270004402096, 0.000543865156368118]\n",
            "[0.0014617869286753591, 0.001398269128098729, 0.001358893265326818, 0.0013896500137412832, 0.0013332598643111331, 0.0013228577351020205, 0.0013412258044506113, 0.0013121953999091472, 0.0013473201232651868, 0.001339354389347136, 0.0013619287294291316, 0.0013500708066636609, 0.0013458807653348361, 0.001343582536182588, 0.0013449849294764654, 0.0013924690528905818, 0.0013629048220103696, 0.0014053593977310119, 0.0013715973789138454, 0.0014057458555769352, 0.0014446591182301443, 0.0014134034713996308, 0.0014454184687652048, 0.0015014694981454384, 0.001449727095175712, 0.0015013347956396284, 0.0014377560610661195, 0.0015552275725418613, 0.0015721454451392803, 0.0015050937177702075, 0.0014805238904608857, 0.0015250692886876918, 0.0015108256877976514, 0.0015204481314867735, 0.001513700404514869, 0.0014817054649548871, 0.0015059082414067926, 0.001620807931093233, 0.0015585669382874455, 0.0015550860203802585]\n",
            "[31.705172952240936, 31.965966164939115, 32.2158002116236, 32.2158002116236, 32.33393640018112, 32.383315890574785, 32.383315890574785, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357, 32.46230391565357]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsrUsJhqFD0b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "7b4d3fbf-5186-4077-aac7-63a9e2fad031"
      },
      "source": [
        "x1=numpy.linspace(1,len(history_Y_5.psnrs),len(history_Y_5.psnrs))\n",
        "y1 = numpy.asarray(history_Y_5.psnrs, dtype=numpy.float32)\n",
        "x2=numpy.linspace(1,len(history_Y_10.psnrs),len(history_Y_10.psnrs))\n",
        "y2 = numpy.asarray(history_Y_10.psnrs, dtype=numpy.float32)\n",
        "x3=numpy.linspace(1,len(history_Y_15.psnrs),len(history_Y_15.psnrs))\n",
        "y3 = numpy.asarray(history_Y_15.psnrs, dtype=numpy.float32)\n",
        "x4=numpy.linspace(1,len(history_Y_20.psnrs),len(history_Y_20.psnrs))\n",
        "y4 = numpy.asarray(history_Y_20.psnrs, dtype=numpy.float32)\n",
        "fig, ax = plt.subplots()\n",
        "ax.plot(x1,y1, '-b', label='5')\n",
        "ax.plot(x2,y2, '-r', label='10')\n",
        "ax.plot(x3,y3,'-g',label=\"15\")\n",
        "ax.plot(x4,y4,'-m',label=\"20\")\n",
        "leg = ax.legend();\n",
        "plt.ylabel('PSNR')\n",
        "plt.xlabel('epoch')\n",
        "plt.show(fig)"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUm5pglBKKeWWJgillFJuaYJQSinl1v8Dm7yBC5hCtp4AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Rsw037fFAeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(num_layers):\n",
        "    VDSR = predict_model(num_layers)\n",
        "    VDSR.load_weights(\"VDSR_ADAM_91\"+\"_\"+str(num_layers)+\".h5\")\n",
        "    IMG_NAME = \"t1.png\"\n",
        "    INPUT_NAME = \"input\"+str(num_layers)+\".png\"\n",
        "    OUTPUT_NAME = \"pre\"+str(num_layers)+\".png\"\n",
        "\n",
        "    import cv2\n",
        "    img = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    shape = img.shape\n",
        "    Y_img = cv2.resize(img[:, :, 0], (int(shape[1] / 2), int(shape[0] / 2)), cv2.INTER_CUBIC)\n",
        "    Y_img = cv2.resize(Y_img, (shape[1], shape[0]), cv2.INTER_CUBIC)\n",
        "    print(img.shape,Y_img.shape)\n",
        "    img[:, :, 0] = Y_img\n",
        "    Y_img=img[:,:,0]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    cv2.imwrite(INPUT_NAME, img)\n",
        "\n",
        "    Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "    Y[0, :, :, 0] = Y_img.astype(float) / 255.\n",
        "    pre = VDSR.predict(Y, batch_size=1) * 255.\n",
        "    print(pre.shape)\n",
        "    pre[pre[:] > 255] = 255\n",
        "    pre[pre[:] < 0] = 0\n",
        "    pre = pre.astype(numpy.uint8)\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2YCrCb)\n",
        "    img[:,:,0] = pre[0,:,:,0]\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_YCrCb2BGR)\n",
        "    print(img.shape)\n",
        "    cv2.imwrite(OUTPUT_NAME, img)\n",
        "\n",
        "    # psnr calculation:\n",
        "    im1_rgb = cv2.imread(IMG_NAME, cv2.IMREAD_COLOR)\n",
        "    im1_Y = cv2.cvtColor(im1_rgb, cv2.COLOR_BGR2YCrCb)\n",
        "    im2_rgb = cv2.imread(INPUT_NAME, cv2.IMREAD_COLOR)\n",
        "    im2_Y = cv2.cvtColor(im2_rgb, cv2.COLOR_BGR2YCrCb)\n",
        "    im3_rgb = cv2.imread(OUTPUT_NAME, cv2.IMREAD_COLOR)\n",
        "    im3_Y = cv2.cvtColor(im3_rgb, cv2.COLOR_BGR2YCrCb)\n",
        "    print(im3_Y.shape,im1_Y.shape)\n",
        "    print(\"bicubic:\")\n",
        "    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im2_Y[:,:,0]),psnr(im1_rgb,im2_rgb)))\n",
        "    print(\"VDSR:\")\n",
        "    print(\"YCrCCb= {} , RGB={}\".format(psnr(im1_Y[:,:,0],im3_Y[:,:,0]),psnr(im1_rgb,im3_rgb)))\n",
        "\n",
        "    # srcnn_model = predict_model()\n",
        "    # srcnn_model.load_weights(\"SRCNN_check.h5\")\n",
        "    # avg_psnr=0.0\n",
        "    # for i in range(0,data_Y.shape[0]):\n",
        "    #     img=data_Y[i]\n",
        "    #     Y = numpy.zeros((1, img.shape[0], img.shape[1], 1), dtype=float)\n",
        "    #     Y[0, :, :, :] = img\n",
        "    #     pre = srcnn_model.predict(Y, batch_size=1) * 255.\n",
        "    #     pre[pre[:] > 255] = 255\n",
        "    #     pre[pre[:] < 0] = 0\n",
        "    #     img_pre=pre[0,:,:,:]\n",
        "    #     avg_psnr=avg_psnr+psnr(label_Y[i]*255, img_pre)\n",
        "\n",
        "    # avg_psnr=(avg_psnr/data_rgb.shape[0])\n",
        "    # print(avg_psnr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVyXT1dtkETe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "2df3ab58-4a23-4b12-b5af-e992008e2b97"
      },
      "source": [
        "print(\"5\")\n",
        "predict(5)\n",
        "print(\"10\")\n",
        "predict(10)\n",
        "print(\"15\")\n",
        "predict(15)\n",
        "print(\"20\")\n",
        "predict(20)"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5\n",
            "(481, 321, 3) (481, 321)\n",
            "(1, 481, 321, 1)\n",
            "(481, 321, 3)\n",
            "(481, 321, 3) (481, 321, 3)\n",
            "bicubic:\n",
            "YCrCCb= 30.656837442172275 , RGB=30.656730765830783\n",
            "VDSR:\n",
            "YCrCCb= 31.199425229183937 , RGB=31.203097277947474\n",
            "10\n",
            "(481, 321, 3) (481, 321)\n",
            "(1, 481, 321, 1)\n",
            "(481, 321, 3)\n",
            "(481, 321, 3) (481, 321, 3)\n",
            "bicubic:\n",
            "YCrCCb= 30.656837442172275 , RGB=30.656730765830783\n",
            "VDSR:\n",
            "YCrCCb= 31.24663870396399 , RGB=31.24625580463001\n",
            "15\n",
            "(481, 321, 3) (481, 321)\n",
            "(1, 481, 321, 1)\n",
            "(481, 321, 3)\n",
            "(481, 321, 3) (481, 321, 3)\n",
            "bicubic:\n",
            "YCrCCb= 30.656837442172275 , RGB=30.656730765830783\n",
            "VDSR:\n",
            "YCrCCb= 31.224088959891443 , RGB=31.229739543119734\n",
            "20\n",
            "(481, 321, 3) (481, 321)\n",
            "(1, 481, 321, 1)\n",
            "(481, 321, 3)\n",
            "(481, 321, 3) (481, 321, 3)\n",
            "bicubic:\n",
            "YCrCCb= 30.656837442172275 , RGB=30.656730765830783\n",
            "VDSR:\n",
            "YCrCCb= 31.22090542637387 , RGB=31.222964272789735\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KFBHQf61oUKp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}